\emph{How can the Explainable AI (XAI) community support public understanding of the spatial workings and effects of civic predictive systems?} 

Civic predictive systems, or systems that affect civic processes, are becoming an inseparable part of the urban smart city.  They inform several fundamental decisions that govern our everyday lives: who gets parole, bail \cite{arnold}, or economic and social aid \cite{noriega2020algorithmic}; who gets hired \cite{raghavan2020mitigating}; how are students assigned to public schools \cite{robertson2021modeling}; how well are teachers performing \cite{teachereval}; which child welfare calls need immediate attention \cite{chouldechova2018case, cheng2021soliciting}; how un-housed people receive housing resources \cite{kuo2023understanding}; who gets a loan application approved \cite{schoeffer2022there}; or which routes we, or our public transports, take in a city \cite{gupta2022rethinking, goodman2019challenge}. They are pervasive. 

The use of predictive systems has been presented as making civic processes more efficient, accurate, and objective. They can effectively and quickly process large amounts of data about a variety of subjects and their environments.  They can identify patterns not previously known while attempting to overcome biases in human-based decision-making \cite{brauneis2018algorithmic}. The hope is that the use of such systems can address the resource constraints within the public sector by off-handing or automating labor, thereby reducing costs and increasing resource allocation capacities \cite{levy2021algorithms}. 

Despite their proposed vision and goals, several civic predictive systems have been shown to cause societal harm and discrimination \cite{eubanks2018automating}. There are countless examples of bias \cite{bartlett2022consumer, dastin2022amazon}, disregard of civil rights and liberties \cite{de2020states, johnson2023face}, or privacy invasions \cite{ice}. The centralized and authoritative nature of algorithms gives them the power to create or reinforce unjust (and often invisible) societal structures and inequalities \cite{d2020data}.  

Geospatial civic algorithms specifically, that make predictions about spaces in the smart city, attempt to fit highly complex cities into neat structures demanded for computation \cite{mattern2020city}. Critical geographers, such as Schwanenand and Kwan, demonstrate how social discrimination and marginalization are inherently entangled with space \cite{kwan2012critical}. The overly simplistic mathematical representation of cities creates the kinds of invisible inequalities and incremental injustices that produce large-scale societal effects, such as gentrification and segregation \cite{loukissas2022wants}. Numerous cases exemplify the inequitable spatial effects on diverse communities:

\leftskip 20pt
\rightskip 0pt plus 1fill
\vspace{8pt}

Planning algorithms such as the Market Value Analysis present governing bodies with a “data-driven objective” means of distributing public resources by classifying and segregating communities through the construction of color-coded boundaries. Such organization of space, as Safransky argues, standardizes unjust classification systems and restructures the public sphere in ways that favor some neighborhoods over others. These civic algorithms are opaque, inaccessible, and are rarely if ever, developed through a public and participatory process \cite{safransky2020geographies}.

Sociologists such as Zukin et al. have demonstrated how geographically coded Yelp reviews reinforce prejudice against neighborhoods with people of color thereby influencing civic investments and contributing to processes of urban change such as gentrification. The algorithmic moderation of reviews is not public, and users are unaware of their role in affecting capital flows \cite{zukin2017omnivore}.

Loukissas argues that filter bubbles created by Zillow reinforce existing spatial power structures along the axes of race and class. Even as the goal of the site is to give users more control over their home-buying experience, it leaves users unaware of the implications of their own filtering decisions \cite{loukissas2022wants}.

In my past work, I illustrate how safe walking apps, such as Safetipin, risk segregating neighborhoods by attempting to advance the safety of one social group while marginalizing another. Once again, knowledge about the failings of social structures and policy normalized by the app is inaccessible to citizens, as well as the app creators \cite{gupta2022rethinking}.

\leftskip 0pt
\rightskip 0pt 
\vspace{8pt}

Without careful consideration of the spatial workings and effects of predictive systems, they risk reproducing or even amplifying historical systems of discrimination. Currently, civic algorithms are deployed in ways that prevent citizens from getting access to them or learning about their existence and effects. This limits the citizens' ability  to contest, oversee, assess, or protest automated decisions that affect their lives in fundamental ways. To address concerns of algorithmic opaqueness and lack of understanding, activists \cite{stop_lapd}, civic organizations \cite{lee2023making,post}, as well as several governmental agencies \cite{white2022blueprint} are calling for advancements in algorithmic transparency, explainability, and impact assessment.  Their goal is to drive visibility into the design of algorithms, opening them up for critique and evaluation by both experts and everyday users.  

\section{Existing XAI and AI Transparency Approaches}

Early research on Explainable AI focused majorly on technical explainablity and transparency. For black-boxed algorithms, post-hoc explanations where another human-interpretable model imitates the practices of a complex model are being designed to understand how an algorithm makes its predictions. Guidotti et al. \cite{guidotti2018survey} categorize post-hoc explanations into these categories: (1) Global Model Explanations where a simpler interpretable model is trained on the same data to approximate the working of the primary more complex model, (2) Outcome Explanations where the focus is on explaining one outcome or instance of prediction by providing the weight of features that contributed to it \cite{schuff2022human} or examples of inputs that would lead to a similar prediction, and (3) Counterfactual Explanations where the goal is to identify what should be changed in the inputs to get a different prediction \cite{shang2022not}. Beyond making models interpretable, XAI efforts attempt to make known other aspects of the machine learning model such as datasets, training algorithms, source code, and performance metrics \cite{vaughan2020human}. Several model \cite{mitchell2019model} and data documentation \cite{anik2021data, bender2018data, gebru2021datasheets, holland2020dataset} frameworks, have been designed to explain AI to experts \cite{dhanorkar2021needs}. Frameworks such as `CrowdWorkSheets' support standardized documentation of decisions when annotating datasets in a crowdsourced manner \cite{diaz2022crowdworksheets}. `Data Cards' provide summaries of datasets for various stakeholders \cite{pushkarna2022data}. To promote transparency of models, existing work proposes tools such as `Model Cards' that aim to provide details about a model related to its working, use cases, and evaluation \cite{mitchell2019model} and `Factsheets' that provide an overview of facts about specific models across the AI lifecycle \cite{richards2021human}. There is also growing work in designing open-source toolkits to identify and assess algorithmic harms and biases \cite{bellamy2018ai, wexler2019if, bird2020fairlearn}. AI Fairness 360 (AIF360) \cite{bellamy2018ai} and Fairlearn \cite{bird2020fairlearn} are tools that aim to help practitioners understand `bias' metrics and allow them to detect algorithmic biases. These tools also help mitigate said biases by providing a variety of mitigation algorithms. Another tool called ``What-If'' uses visualizations to help users and practitioners investigate how a model will perform in hypothetical scenarios created by changes in data points \cite{wexler2019if}.

More recently, the XAI community has presented the need to make AI processes visible to the general public in order to build trust in the artificially intelligent systems that guide their lives \cite{knowles2021sanction}. There has been a growth in work that focuses on developing methods and frameworks for user-centered algorithmic explanations \cite{miller2019explanation, shneiderman2020human, vaughan2020human}. Human cognitive abilities \cite{wang2019designing}, users' explanatory needs \cite{liao2020questioning, springer2020progressive}, users' situated real-world experiences \cite{devos2022toward}, users' ability to collaborate and form counter publics \cite{shen2021everyday} have been some of the guiding factors in advancing user-centered XAI research.    

Such efforts have built on diverse theories and methods: interactive explainability methods \cite{cheng2019explaining}, such as Interactive Model Cards \cite{crisan2022interactive}, have been proposed, theories describing human cognition patterns have been employed \cite{wang2019designing}, and example-based methods where data samples that informed a prediction are shown to users have been presented as helpful \cite{cai2019effects}. Additionally, several toolkits have been designed to promote user understanding of AI systems such as: TILT (transparency information language and toolkit) that organizes the information that transparency policies demand in structured ways for machines to read and users to consume \cite{grunewald2021tilt}, or AIX360 toolkit \cite{arya2019one} that present visualizations and explanation algorithms respectively to help users understand how predictive systems work. Investigations into how XAI can support user assessment \cite{robertson2022understanding} and decision-making capabilities \cite{kulesza2013too} are also being conducted.

These efforts have made monumental progress in the field of Explainable AI. Yet, there remain limits. First, they are not pluralistic in nature \cite{hancox2021epistemic, ehsan2022social} and do not consider the values, surrounding social systems, existing knowledges, and beliefs \cite{kaur2022sensible} of users in the design of explanations. Second, they may focus merely on technical transparency, disregarding the systemic factors surrounding any algorithmic decision \cite{ehsan2021expanding}. Third, they tend to be one-time explanations that portray users as passive individual consumers \cite{corbett2023interrogating}. And lastly, the goal of most explanations is to increase user trust rather than to promote critical thinking or action \cite{danry2023don}.  This leaves little room for democratically evaluating how the complex world we live in is simplified to be represented in the design of algorithms— who is included, who is excluded, and how cities are quantified and aggregated for computation. I discuss these limits in more detail in \ref{ch2:gee}. 

This dissertation aims to address these limits and theorize the design of effective public explanations of civic AI tools. I want to note that in using the term `explanations', I am not referring to the technical explanations provided through the use of algorithmic interpretability methods. Rather, I am attempting to expand our community’s understanding of `explanations' such that it is not limited to the technical understanding of the systems but is explaining the social, political, and economic development, use, and effects of civic predictive systems. 

I ground my work in public safety AI systems. Specifically, I employ place-based predictive policing as a case study for this research.


\section{Public Safety Algorithms and Harms }  

My research for this dissertation started with a critical analysis of a renowned safe walking app, primarily deployed in India, called `Safetipin' \cite{safetipin}. Safetipin recommends `safe' paths to users from an origin to a destination by calculating `safety scores' for various paths. These safety scores are calculated by aggregating crowdsourced `safety data' such as the amount of lighting, or presence of security officers, in various locations in a city. To study this tool, (1) I draw on feminist criticisms of safety technologies, defined broadly, to identify the main issues in their framing and efficacy, and (2) I build upon initial interviews with users and makers of Safetipin to examine how the app addresses, or fails to address, the criticisms received by other safety technologies.

This critical analysis demonstrates Safetipin’s capacity to (1) restrict women’s movement to computationally calculated `safe' neighborhoods and (2) reinforce caste and religion-based segregation in India \cite{gupta2022rethinking}. By disregarding the prejudice about vulnerable neighborhoods that governs the `feeling of safety' of its users who contribute to the crowdsourced data, Safetipin fails to situate itself in the broader historical politics of safety in the city \cite{kern2021feminist} that continue to marginalize people of lower socioeconomic status and minority religions. Nonetheless, the app and its underlying information infrastructures that promote segregation, have been enthusiastically accepted and celebrated \cite{safetipin_award1, safetipin_award2}. This work presents a dire need to identify and understand the impact of spatially distributed data inputs and aggregations on the city and its people. Ultimately, it motivates the need to explain how emerging civic geospatial technologies, especially in the realm of public safety, organize cities and their impact on spatial segregation and discrimination. 

Amongst other tools for public safety exist a variety of ML algorithms that have been developed with the hope to mitigate crime and advance citizen safety in smart cities. Popular examples include— COMPAS \cite{compas}, which predicts recidivism risk for an individual; Predpol (now Geolitica) \cite{predpolbad}, which predicts geographic areas where crime is most likely to happen; Arnold Public Safety Assessment \cite{demichele2020public}, which provides judges with sentencing recommendations. These algorithms, even as they aim to promote public safety in cities, tend to reinforce discrimination along the axes of race and class. Jefferson demonstrates how Predpol legitimizes the bias embedded in official crime datasets and has resulted in the over-policing of already heavily surveilled neighborhoods \cite{jefferson2018predictable}. Risk assessment tools build upon and reinforce the racist policies and infrastructures underlying carceral systems in the US. Additionally, they define `risk' at the level of an individual, disregarding how `risk' is a reflection of societal prejudice against various social groups \cite{green2020false}. In India, centralized systems and norms along with the subjectivities of individual police officers lead to historical, representational, and measurement bias in recorded crime data for the Crime Mapping Analytics and Predictive System (CMAPS) predictive policing tool. Further, the opaque design of CMAPS allows for discrimination against immigrant colonies and minority settlements by promoting the belief that crime rises in specific neighborhoods by virtue of the above-mentioned communities living there  \cite{marda2020data}. Transparency is a much-needed feature for the effective assessment and development of public safety algorithms \cite{rudin2020age}. Given the limited potential of techniques designed to “de-bias” public safety algorithms, there is an urgent need to make the data assemblages \cite{kitchin2014towards} surrounding public safety algorithms transparent and accessible to city residents and governmental bodies \cite{marda2020data}.   

This dissertation aims to study the concept of effective public explanations,  by specifically focusing on place-based predictive policing as a case study. Place-based predictive policing is a method that aims to support the efficient distribution of police resources in a city. Typically, the method utilizes historic crime data such as arrest reports or calls for service requests integrated with other social and environmental data to predict the location and time of future crimes. A popular example of such a system is called Geolitica (previously Prepdol) \cite{predpolbuy}. In the past decade, academics and activists have heavily scrutinized the use of predictive policing \cite{o2017weapons}. The tool has been shown to reproduce existing geographic and social biases embedded in historically discriminatory crime data \cite{jefferson2018predictable,akpinar2021effect}. A recent study conducted by the Markup has found the accuracy of this tool to be less than half a percent \cite{predpolbad}. Many others have discussed the incompleteness of data \cite{kirkpatrick2017s}, the inability to validate results \cite{demortain2017evaluating}, and the proliferation of positive feedback loops \cite{o2017weapons}. Such thorough investigation of this predictive tool makes it an ideal case to ground our discussions of effective public explanations. 



\section{Research Questions and Chapter Outline}

In this dissertation, I ask: \emph{How can the XAI community support public understanding of the spatial workings and effects of civic predictive systems (RQ)?} To investigate this question, I primarily employ community-centered qualitative and participatory methods. Additionally, I build on and am in conversation with scholarship from fields such as human-computer interaction (HCI) that investigates how users perceive, relate to, and interact with AI systems to seek AI explanations; science and technology studies (STS) that shines light on the socio-technical environments surrounding AI, publics affected by AI, as well as the processes of explaining and knowing; and AI Transparency and Explainability (XAI) that has designed numerous methods, frameworks, and tools to explain AI systems to a variety of audiences. 

\Cref{ch2:gee} begins my investigation of the primary research question guiding this dissertation (RQ) by asking: \emph{ What qualities underlie effective public explanations of civic predictive systems (RQ1)?} My inquiry is driven by a semi-structured interview study along with an extensive review of existing scholarship on public explanations for civic predictive systems. I interview 23 participants including academics, AI activists, journalists, community and neighborhood leaders, and civic society organizations who think, write, or act on issues of social justice and AI safety, to identify the qualities and characteristics underlying meaningful public explanations of civic predictive systems. Drawing on my findings, I introduce the concept of \emph{ ‘good enough explanations’}. ‘Good enough explanations’ as understood by our participants, (1) are \textit{situated} in the lives of diverse publics, (2) explain the complex and entangled socio-technical \textit{systems} that predictive tools interact with, (3) involve \textit{continuous and partial} processes, and lastly (4) empower publics to \textit{act} in ways that promote democratic deployment and regulation of predictive tools. Such explanations, I argue, may not be complete or objective but are good enough to support publics in critically engaging with the workings and effects of civic predictive systems in service of their goals. The rest of the dissertation attempts to understand and explore these qualities.  

\Cref{ch3:methods} documents my approach to study, as well as create, good enough explanations. \textit{Form} has been considered an essential dimension of AI Explanations \cite{van2021effect}. As such, I take inspiration from the visual history of city representation to demonstrate how \textit{mapping} may allow us to explain geo-spatial AI systems in ways that are accessible, culturally reflexive, situated, and provide visibility into how algorithms represent cities. I employ a `research through design' methodology and conduct participatory mapping workshops with diverse publics such as— police reform groups, city planners, neighborhood and community leaders, civic development agency, and educators— to investigate how can we, as XAI researchers, design good enough explanations. The workshops encouraged participants to question, understand, and explain place-based predictive policing on their own terms by using maps to ground discussions in the spaces they live and work in. The explanation contexts that emerged were analyzed using inductive coding, situational analysis, memo-ing, and thematic analysis. These workshops support the findings and arguments developed in chapters 4, 5, and 6.

\Cref{ch4:situated} studies the `situated' nature of public explanations and asks: \emph{How are explanations situated in the lives of diverse publics and what does that mean for the design of public explanations (RQ2)?} Drawing on five participatory mapping workshops with diverse publics, this chapter reflects on the explanation contexts that emerge when publics question or understand AI systems. I find that when attempting to understand AI systems, publics draw on their relation with not just the predictive technology, but also the prediction domain, prediction subject, and prediction backdrops. Situated explanation needs of publics emerge out of these entangled relations. I argue that these broader relations play a significant role in supporting public understanding of civic AI systems and urge XAI researchers to consider these relations as they attempt to design systems for public understanding of AI. 

Chapter 5 studies the `systemic' nature of public explanations and asks: \emph{How can the XAI community help explain the complex socio-technical systems that civic AI tools engage with?} Underlying AI systems are socio-technical assemblages of materials, relations, cultures, institutions, and histories. In this chapter, I analyse the participatory mapping workshops using situational analysis to demonstrate the ability of local publics to partially explain the environments AI tools are deployed in, the cultures and norms they invade, and the lived experiences of the problems they attempt to address. Drawing on these findings, I argue that good enough explanations need not be designed in isolation \emph{for} publics \emph{by} XAI researchers. Instead, XAI researchers would benefit from co-creating systemic explanations \emph{with} diverse local publics through slow and long-lasting engagements.  

Chapter 6 reflects on the methods I use to develop good enough explanations to reconceptualize explanations as explain\emph{ing}— a continuous and partial process that supports the development of \emph{situated} understandings of AI \emph{systems}. I provide a detailed account of three design dimensions I believe should be considered as XAI researchers design spaces and systems that mediate processes of explaining: (1) designing explaining sites (2) designing explaining media, and (3) designing explaining interactions. I briefly discuss how explain\textit{ing} may influence public `action'. I end by providing a list of questions that XAI researchers can consider as they attempt to create good enough explanations for civic predictive systems.  

Lastly, I conclude by providing a summary of the research conducted in this dissertation, highlighting limits of this work, proposing relevant future work, and positioning the work in relation to my background and motivations. 

Together, these chapters attempt to make three primary contributions to the field of XAI: (1) theoretical contribution by conceptualizing what \emph{good enough explanations} mean for public understanding of civic predictive systems, (2) empirical contribution by reporting how diverse publics may understand, question, and explain civic predictive systems, and (3) methodological contribution by demonstrating and recommending strategies for co-creating a good enough understanding of civic predictive systems.  

My hope is that this work will support XAI researchers, policymakers, and community leaders in designing systems and spaces for public engagement with civic predictive systems. Such engagements, as stated in \Cref{ch2:gee}, should work to promote democratic public action toward contesting, redesigning, regulating, or discontinuing predictive systems that may cause societal harm in fundamental yet currently invisible ways.  

 