\subsection{Explanations needs for diverse AI expertise}

Existing knowledge and understanding of AI have been used as a metric to conceptualize user-centered explanations. Researchers relate low literacy levels in non-expert users to higher unwarranted trust in AI systems, thereby calling for advancements in user understanding of AI \cite{ehsan2024xai,okolo2023navigating}. 

Users' AI expertise is understood in many different ways. Some works clearly define user expertise using scales or metrics. In their survey, Hohman et al. categorize the audience of XAI as 'Model Developers and Builders', 'Model Users', and 'Non-Experts' with decreasing levels of AI expertise \cite{hohman2018visual}.  Similarly, Yu and Shi report on the goals of users based on whether they are beginners, practitioners, developers, or experts \cite{yu2018user}.  Mohseni et al. focus specifically on data literacy and add 'Data Experts' as a category to define user expertise \cite{mohseni2021multidisciplinary}. 

Some researchers have used technical means to codify user expertise into their explanation platforms. Rong et al. propose a framework called Image Classification Explanations or I-CEE encoded in an explanation tool, which provides explains AI to users by providing them with a sub-set of training data based on their expertise. The user expertise is categorized as an m-dimensional vector that informs the explanation provided to them \cite{rong2024cee}.  Other researchers have however chosen participatory ways to investigate what to explain and how to explain such that users' mental models are more closely aligned with expert mental models \cite{eiband2018bringing}.

Ehsan et al. emphasize the need to consider 'who' is attempting to understand AI, arguing that users' underlying characteristics, such as their AI background, motivate their explanation needs. They demonstrate how groups with different understandings of AI, such as Computer Science students versus Amazon Mechanical Turk (AMT) participants, use diverse heuristics and appropriation techniques when engaging with explanations \cite{ehsan2024xai}. 


\subsection{Explanation needs for diverse user roles}

Depending on the type of stakeholder a user is, and what their functional role is in relation to the predictive tool, they may have different explanation needs. Tomsett et al. propose six different roles namely: Creators, Operators, Executors, Decision-subjects, Data-subjects, and Examiners. They speculate explanation needs for these personas such as optimizing models for creators or contesting decisions for decision subjects \cite{tomsett2018interpretable}. Preece et al. expand the binary distinction of developers and users to also consider the explanation needs and motivations of theorists and ethicists. They suggest that a ‘layered’ approach to explanations can meet the needs of diverse stakeholders \cite{preece2018stakeholders}. Hong et al. focus specifically on ML practitioners and identify explanation needs for the roles of model builders, model breakers, and model consumers. They emphasize that explanation needs vary with contexts and depend not only on stakeholder roles in relation to the predictive tool but also on relations between workers in an organization \cite{hong2020human}. 

These works demonstrated how explanations needs are conceptualized in relation to users' AI expertise and functional roles. They presuppose the explanation contexts of users in relation to their expertise and roles and design systems and tools for such contexts. Even as these approaches show promise for the design of user-centered AI explanations, how they conceptualize explanation contexts is reductive. 

\subsection{Towards a more holistic understanding of users' explanation contexts}

The categorizations of user expertise and roles as described above remain limited. The knowledge of the audience is merely evaluated based on their technical expertise, disregarding other knowledges that may affect their understanding and explanation needs such lived experiences. Additionally, their explanation needs are singularly considered a factor of their presumably static roles \cite{suresh2021beyond}. In response, Suresh et al. present a framework to granularly describe stakeholders’ knowledge and interpretability needs. The first part of their framework focuses on stakeholders’ expertise and describes two dimensions: stakeholder knowledge (formal, instrumental, and personal knowledge) and the contexts in which this knowledge manifests (i.e., machine learning, the data domain, and the milieu). The second part describes stakeholder needs through a three-level typology: long-term goals (understanding the model, and building trust in it), shorter-term objectives, (debugging a model, or contesting a decision) and immediate and specific tasks (assess prediction reliability and detect mistakes). Their framework emerged out of a broad survey of interpretability and pedagogy literature \cite{suresh2021beyond}.

Researchers have also highlighted the need to study user explanations not in an individual context but in relation to the contexts and communities that surround them.  Kou and Gui draw on Activity Theory  and present a framework of six components and inter-relationships namely: subjects (peoples and platforms involved in explaining), tools (that mediate the process of explaining), division of labor (who is explaining), rules and community (circumstances within a platform), and objects (contextualizing AI) to seek socially oriented, systemic-oriented, and mechanism-oriented explanations \cite{kou2020mediating}. 

While works on nuancing explanation contexts is very nascent, these works present some ways forward to develop a more holistic outlook of explanation contexts. I build on these works to define explanation contexts more precisely in this chapter.  