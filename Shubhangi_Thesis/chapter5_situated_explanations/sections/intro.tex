For the public to be able to effectively use, manage, oversee, assess, or challenge pervasive AI systems, they must understand the workings and roles of AI in their lives. Earlier efforts to explain AI focused on AI developers to support them in debugging their models. However, more recently, policymakers and researchers are calling for human-centered Explainable AI, i.e. creation of explanations in ways that meet the needs of users, including everyday publics. 

Current human-centered XAI work considers explanation needs to vary based on users' relations with predictive tools across two dimensions:  (1) AI literacy: the pre-existing knowledge people have about AI tools, and (2) Roles: their position as a stakeholder such as user, auditor, policy maker, etc.  Ongoing investigation into both these dimensions is necessary to understand how users' explanation needs come to be. However, it may not be sufficient. As Nicenboim et al. argue, explanations are contextual \cite{nicenboim2022explanations}. Such contexts can be broad-ranging and include rich and nuanced relations with other aspects of the algorithmic ecosystem. These contexts, as Paul Dourish argues, cannot be pre-supposed. They are not defined by a set of stable conditions that exist outside the action of people. Instead, they emerge out of the course of action. As such, they can only be reflected upon in hindsight. I term the contexts that emerge in the process of explaining AI— `explanation contexts'. Actors, materials, knowledges, and relations that become relevant to questions and understanding AI become part of continuously re-negotiated explanations contexts. Such a holistic study of explanation contexts remains understudied. To that end, this chapter asks: \emph{How can we understand the explanation needs of  publics' as situated in diverse explanation contexts?} 

To answer this question, I draw on the five participatory mapping workshops I detailed in \Cref{ch3:methods}. These workshops invite participants to engage in the processes of questions and knowing place-based predictive policing systems. I reflect on these workshops to understand how explanation contexts reveal themselves and how users' explanation needs emerge from dimensions beyond what has been identified by existing literature (i.e. literacy and roles). I employ Clarke's ‘situational analysis’ \cite{clarkesituational} to identify how publics relate to algorithmic contexts as they seek explanations. 

We find that publics relate to algorithmic contexts through (1) the predictive domain: the service domain that an AI model becomes a part of— in our case policing, (2) the prediction subject: the people or places that are subject to predictions—in our case neighborhoods, (3) the predictive backdrop: the local and global environment that surrounds predictive systems, and (4) the predictive tool: the tools and models that make predictions. I argue that publics explanation needs emerge out of their personal or communal relations with these elements. While existing research in user-centered XAI has majorly focused on the fourth element—relation to the predictive tool through user literacy or role— I suggest that understanding some of these other relations can help XAI researchers provide meaningful explanations situated in the lives of diverse publics.  

In what follows, I provide a summary of human-centered XAI research and how they understand algorithmic context in section 2. Next, I describe my use of situational analysis and situational maps to analyze the workshops in section 3. In section 4, I report on the explanation contexts that emerge in three of the five workshops I conducted. Lastly, I propose a framework that can help XAI designers in considering the broader relations through which users' explanation needs emerge. 