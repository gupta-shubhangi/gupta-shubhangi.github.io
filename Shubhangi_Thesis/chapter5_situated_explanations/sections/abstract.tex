We established in \Cref{ch2:gee} that good enough explanations are situated. This chapter explores this quality in greater depth and asks: \textit{How can we understand the explanation needs of diverse publics as situated in their broad-ranging contexts?} As the emerging field of Human-Centered Explainable AI (XAI) aims to design algorithmic explanations for diverse users, they consider user needs to vary along two dimensions: (1) users' AI literacy levels and (2) users' roles as stakeholders in the AI ecosystem. While considering the relationship between users and predictive tools, defined by their literacy and roles, is necessary for the design meaningful explanations, it is not sufficient. This chapter demonstrates how publics' explanation needs emerge out of broader relations with algorithmic contexts. To do so, I report on the five participatory mapping workshops detailed in the previous chapter. These workshops with diverse groups were aimed at collectively questioning and understanding AI systems. I analyze how publics relate to the contexts around them as they seek to understand predictive systems. I find that publics' explanation needs emerge not just out of their relation with the predictive tools but also through their relations with predictive domains, predictive subjects, and predictive backdrops. As such, I urge human-centered explainable AI designers to consider these relations as they design systems to support public understanding of AI. 
