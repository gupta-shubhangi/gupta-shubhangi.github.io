In this section, I interpret the findings of this study, alongside relevant scholarly literature, to identify qualities and concepts underlying effective explanations of civic AI. I term such explanations `good enough explanations' as they may not be `complete' or `universal', but are good enough for the public to consciously and critically engage with civic AI. I draw on real-world public explanations of predictive tools for policing to exemplify how these qualities may play out. This conceptualization of good enough explanations aims to be generative and these examples only serve as starting points for further developing these concepts.

\subsection{Good Enough Explanations are Situated} 

Stakeholders in this study describe how explanation needs differ based on who consumes an explanation and in what contexts \cite{dourish2004we}. Focused on the realm of civic AI, they add nuance to current works that challenge the universal nature of explanations \cite{dodge2019explaining}. User-centered XAI scholarship has also called for the design of explanations based on user expertise, personas, beliefs \cite{miller2019explanation}, existing knowledges \cite{brennen2020people}, and goals \cite{liao2021human}. Additionally, local publics themselves offer unique and local explanations of how civic AI systems may work \cite{d20205}. Their role can be extended from receivers of explanations to co-creators.

As such, this study understands good enough explanations to be situated in the lives, experiences, and settings of diverse publics. Good enough explanations are good enough for \emph{someone} and thus are defined in relation to their contexts. Determining who an explanation should choose to center is then an important, yet often neglected, choice \cite{brennen2020people}. I highlight the need to especially center the needs and knowledges of communities most at risk of being affected by civic AI.    

In Oct 2023, an article by Aaron Sankin and Surya Muttu was co-published by the Mark and the Wired \cite{wiredmarkup}. It describes their examination of 23,631 Geolitica predictions and calculates a success rate of less than half a percent. Here, I see two examples of good enough explanations: one provided \emph{to} the writers of the article and the other provided \emph{by} them. The writers discuss how they had limited transparency as only two out of thirty-eight police departments, that they reached out to, provided patrolling data, that ultimately informed their analysis. Yet, the partial explanations allowed them to launch an inquiry into the efficacy of the tool for specific cities. On the other hand, their analysis of the tool can now serve as a good enough explanation for police departments looking to make a decision about the potential adoption of predictive policing. 


\subsection{Systemic} 

When asked what participants in this study think diverse publics may want to know about civic AI, they brought up several financial, historical, political, and social aspects of an AI system. Fellow scholars have called for the design of explanations that go beyond the technical aspects of the AI system \cite{birhane2019algorithmic} and focus on explaining socio-technical elements of AI \cite{knowles2022many}. They have highlighted the need for an explanation of `milieus', the environment, where the human-AI interaction takes place \cite{suresh2021beyond}.  

As such, this study understands good enough explanations to be \textit{systemic}. They do not merely aim to explain the technical aspects of a predictive tool. Rather, they make known a more systemic view of the tool including its financial, historical, political, and social aspects. 

Baykurt et al. compare New York City's (NYC) and Seattle's civic efforts that focus on algorithmic transparency and algorithmic impact assessment respectively \cite{baykurt2022algorithmic}. They highlight the problems NYC ran into and suggest that since Seattle's efforts were not focused on opening up the black box of predictive systems, they were able to better investigate the community-based harms associated with predictive tools. Yet, they consider explanations good enough when they can consider other systemic factors such as the social roles and power relations within the agencies that deploy these tools. This may help overcome the need to open up opaque AI systems. Several other public explanations of AI tools for policing highlight systemic assemblages surrounding predictive models, explaining the origins, financial costs, and effects of the tools \cite{lucy}. 

\subsection{Ongoing and Partial} 

When attempting to understand civic AI, stakeholders in this study seek parts of an explanation delivered over extended periods. Meaningful transparency has been suggested to be a ``never-ending endeavor'' \cite{eyert2023rethinking}. Understanding of an AI system develops over time— ``through repeated exposure and interactions'' \cite{suresh2021beyond}. It may last beyond the product's lifecycle throughout people's lives in the form of AI literacy efforts \cite{kelley2023advancing}. Such slowness, over a longer period of time, invites reflection, develops critical understanding, and supports the public in actively thinking about their interactions with technological artifacts \cite{hallnas2001slow}, while reducing over-reliance and trust in automated predictions \cite{buccinca2021trust, park2019slow}. Additionally, stakeholders suggested the need for limited, but relevant, explanations. As Miller describes, people tend to select a few primary causes as an explanation rather than desire a \emph{complete} explanation \cite{miller2019explanation}. Effective explanations provide appropriate and usable information, contingent on publics' existing knowledge and goals \cite{liao2021human}. Explanations need not always be neat, structured, and comprehensive representations of complex entangled predictive systems \cite{inman2019beautiful}.

Good enough explanations, therefore, are slow, ongoing or continuous and partial. Their goal is not to provide a one-time snapshot of how the predictive systems work in their entirety. Instead, good enough explanations may last several interactions and are intentionally partial.  

Atlas of Surveillance \cite{atlas}, developed by the Electronic Frontier Foundation and the University of Nevada, Reno Reynolds School of Journalism, provides transparency into the technologies used by various police departments in the United States. Volunteers, slowly, but steadily, crowdsource information from media posts, press releases, government meeting agendas, and news articles to bring together regularly updated information. Today, as they report, they have over 10,000 data points that have been used to study how policing technologies are growing. Even though this information is incomplete and continuously growing, it has been highly impactful \cite{atlas_impact}. It has provided important context to several research studies \cite{duxbury2022boys} and has been good enough to launch investigations into the use of technologies by specific law enforcement agencies \cite{atlas_markup}. 

\subsection{Actionable}  

Participants describe meaningful explanations to be a means to an end, rather than an end themselves. Explanations serve as starting points for severals purposes including but not limited to: accountability, discussion, reflection, assessment, and regulation. Researchers have critiqued the ideal of transparency alone \cite{ananny2018seeing}. They see the potential of transparency in supporting informed citizens to act \cite{lima2022conflict} and impact the governance of technology companies through their roles as shareholders and consumers \cite{adams2019stimulate, selwyn2022australian}. The ability to act is key to effective explanations \cite{bhatt2020explainable}. 

As such, good enough explanations are good enough if they are surrounded by mechanisms that allow social groups to act. They serve as precursors to other goals. The design of good enough explanations then includes the design of systems and infrastructures that promote action \cite{geiger2023rethinking}.  

One promising grounds-up initiative for oversight over police technologies is called Community Control of Police Surveillance (CCOPS) \cite{ccops}. This movement aims to provide ongoing transparency and control to community members over if and how city agencies acquire or use surveillance technologies. The adoption of CCOPS ordinances, and the transparency that it has provided, has resulted in the ban of oppressive technologies, such as facial recognition, in several states including California and Massachusetts \cite{ccopsstop}. In New York City, however, the adoption of CCOPS has yielded limited results in large part due to systemic barriers, which need to be overcome for transparency to be good enough. Knowledge of a lack of an explanation can also be good enough for specific contexts. Andrew Guthrie Ferguson, author of `The Rise of Big Data Policing: Surveillance, Race, and the Future of Law Enforcement' mentions in an interview that when the public asked questions about policing technologies, police officers did not have good answers \cite{interview}. Eventually, departments realized that predictive technologies may not be effective or worth the money. Explanations and transparency are only good enough in so far as they can inform the regulation of and control over the use of civic AI.

These qualities—situated, systemic, continuous and partial, and actionable—come together to formulate  `good enough explanations'. The qualities are not mutually exclusive or exhaustive. The questions asked by the participants and the related qualities I articulate, highlight that the process of creating an explanation for a civic predictive technology can be more important than the veracity of the explanation itself. Ultimately, this study defines good enough explanations as \emph{continuous and partial processes} that allow \emph{diverse publics} to engage with \emph{features of predictive systems} and assess such systems in relation to their communities in \emph{service of their goals}. 


 