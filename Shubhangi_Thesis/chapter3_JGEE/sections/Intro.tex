As predictive systems increasingly inform civic decision making \cite{robertson2021modeling, kuo2023understanding, cheng2021soliciting}, activists, academics, as well as policymakers are calling for public involvement in their assessment and regulation \cite{balaram2018artificial, eyert2023rethinking, billofrights}. However, a limited understanding of the existence, workings, and impacts of predictive systems has made it challenging for citizens to democratically participate in the decision-making surrounding the use of these tools \cite{eyert2023rethinking}. As described in \Cref{ch1:intro}, Artificial Intelligence (AI) explainability, transparency, and interpretability methods have been proposed to promote public understanding of AI systems. However, recent scholarship has demonstrated the limits of existing approaches for meaningfully supporting public knowledge, usage, and assessment of civic AI systems \cite{corbett2023interrogating, eyert2023rethinking}. They discuss how current methods, while influential, (1) design for individual users as static and universal consumers of AI explanations disregarding their pluralistic and situated perspectives, (2) aim to merely provide technical explanations of AI separate from the broader socio-political systems that surround it \cite{ehsan2021expanding, sloane2023introducing},  (3) are one-time explanations that may be overwhelmingly lengthy or convoluted, and (4) aim to develop user trust rather than problematize trust and develop critical thinking or promote action \cite{gupta2023mapping, harrison2001trust}. These limits highlight the need for the Explainable AI (XAI) community to center local publics in the study and design of effective explanations.  

To address this epistemic challenge, this chapter asks: \emph{What qualities underlie effective public explanations of civic predictive systems (RQ1)?} In this paper, I report on twenty-three interviews with people who I, along with my collaborators, see as stakeholders in civic AI, such as academics, journalists, and leaders in civic organizations and neighborhood associations, all of whom seek pragmatic explanations for the predictive technologies that are poised to change the communities in which they work. As I have mentioned, this dissertation focuses specifically on civic predictive systems, i.e., predictive systems used for civic purposes, because of their omnipresent yet concealed nature. The lack of direct interaction with such technologies makes democratic oversight and assessment especially difficult, thereby presenting an urgent need for investigation. Specifically, I focus on place-based predictive policing and its impacts, that have been studied at length making it a rich case to help build our understanding of effective explanations \cite{brayne2020predict, ferguson2016policing, purves2023machine}.

Participants in this study identify four questions as imperative for creating effective public explanations: (1) who receives and creates an explanation; (2) how is an explanation developed and shared; (3) what does an explanation explain; and (4) what are the goals and impacts of an explanation. This chapter, including my report of current XAI limits in the following section, is structured around these four questions.

I also identify three challenges faced by the broader XAI community in designing effective explanations: (1) limited access to information about civic predictive tools, (2) lack of awareness, interest, or availability amongst the public to engage with predictive tools, and (3) lack of consensus on best ways to regulate tools and mitigate their harms.  

Through an interpretive analysis of the findings, I conceptualize effective public explanations as processes (1) that are \textit{situated} in the lives of diverse publics, (2) explain the complex and entangled socio-technical \textit{systems} that predictive tools interact with, (3) involve \textit{ongoing and partial} processes, and lastly (4) empower publics to \textit{act} in ways that promote democratic deployment and regulation of predictive tools. I bring these dimensions together by introducing `good enough explanations' as a tool that embodies these values. I borrow the concept of `good enough' from the work of Donald Winnicott, a pediatrician and psychoanalyst, who coined the term `good enough parenting' \cite{winnicott1991playing}. He contrasts a good enough parent with a perfect parent and notes that a good enough parent does not meet every need of their child. This, he argues, helps their child be more resilient. I want to highlight that Winnicott conceptualized the term `good enough' in the 1950s, a time before the second wave of feminism, when discriminatory gender roles were condoned by the society. His work, then, characterized women as the sole care taker of an infant in a nuclear family \cite{gerson2004winnicott}. What I take from him work, however, is his semantic conceptualization of `good enough'. The term `just good enough' was also recently used by Gabrys et al. \cite{gabrys2016just} who introduced `just good enough data' to describe citizen data created via limited means and methods that may offer new ways to relate to data and mobilize them. Since algorithmic explanations too cannot be perfect \cite{sloane2023introducing}, this chapter presents good enough explanations as explanations that are not `complete' or `universal', since that remains impossible, but are good enough for diverse publics to consciously engage with civic AI systems. My hope is that such explanations can help overcome the epistemic barriers presented by opaque algorithms \cite{tseng2023assemblage}. Ultimately, I define good enough explanations as \emph{ongoing processes} that allow \emph{diverse publics} to \textit{partially} engage with \emph{features of predictive systems} and \emph{assess such systems in relation to their communities}.

In what follows, I provide a brief background on the limits of current Explainable AI (XAI) and AI transparency work, that motivates this chapter. Next, I describe the methods that this chapter employs, followed by the findings. The findings section also includes a description of the challenges presented in developing effective explanations. Lastly, I end by analyzing the findings to propose and define the concept of good enough explanations.