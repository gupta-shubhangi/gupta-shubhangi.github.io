AI systems are not explainable in the binary of yes or no. Instead, this chapter attempts to conceptualize a `good enough' explanation of civic AI. An understanding of such explanations is informed by the interviews I conducted with people who I believe to have a stake in the workings and effects of civic predictive tools such as academics, journalists, and leaders in civic organizations and neighborhood associations. I argue that good enough explanations arise through processes that allow diverse publics to assess features of predictive systems in terms of the effects on their communities. 

In conclusion, this chapter contributes to existing XAI and AI transparency scholarship in the following ways: (1) it focuses specifically on studying public needs for effective explanations of civic AI systems, (2) it foregrounds the perspectives of non-tech experts and local publics who may have a stake in the design and use of civic AI systems, (3) it organizes empirical and theoretical knowledge to define what effective public explanations could look like.  

My next step in this dissertation is to study systems that can support the creation of situated, systemic, continuous and partial, and actionable explanations. To that end, and motivated by research that presents the \textit{form} explanations may take as an influential component of public explanations, I explore if and how participatory mapping can be used to study and create good enough explanations for diverse publics. In the next chapter, I demonstrate the affordances offered by participatory mapping as an explanation technique, followed by an account of the participatory mapping workshops I conducted in the course of this dissertation research. 
