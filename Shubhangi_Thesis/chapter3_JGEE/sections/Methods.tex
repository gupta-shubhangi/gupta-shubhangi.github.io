This study draws on twenty-three semi-structured interviews with people from academia, civic organizations, activist communities, non-profits, journalism, neighborhood associations, etc. (described in detail in \Cref{table21}). All participants that were interviewed have written, thought or acted extensively on issues of predictive systems, criminal justice, social justice, and city life. They were chosen due to their presence and experience as members or leaders of communities with stakes in the harms caused by civic predictive tools. They were identified through a broad search for stakeholders that could speak to these issues on a local or national level in the United States. They were recruited through direct contact and referrals.  

The interviews lasted a total of thirty to ninety minutes. The study was performed in Atlanta, GA in the United States. These interviews included, but were not limited to, the following discussion prompts: 

% TODO(@nirbhay): Insert table here. 3 columns, 24 rows.
\input{chapter3_JGEE/tables/table1}

\begin{itemize}
    \item What, if any, is the role of citizen-centered AI transparency in the design and deployment of just civic predictive tools? What is at stake and why? What are the challenges? Why? 
    \item What is needed to promote meaningful citizen-centered transparency? Is partial understanding enough? What does democratic control over AI look like?
    \item Do you or your association think about AI use by cities? Why? Why not? Do you think there is a need to do that?
    \item What do you know about the use of AI for public safety in your neighborhood? What questions do you have? What forums exist to offer information on the use of AI by cities?
\end{itemize}

The interviews were either (1) conducted over Zoom and transcriptions of the interviews were recorded [P1-P12, P18-P23],  (2) conducted over the phone with active note-taking [P13-P16], or (3) conducted in person with active notetaking [P17]. The decision to not record audio or video during the interviews was taken after a pilot interview where I found that recording the interview was making participants uncomfortable or hesitant in their responses.  

\subsection{Data Analysis}  

Following the thematic analysis method proposed by Braun and Clarke, I first familiarized myself with the transcriptions and notes. Next, I coded the data using an inductive method identifying both semantic and latent codes such as: ‘explanation needs vary with stakeholders, ‘explanation should promote local expertise’, ‘explanations should provide information about procurement’, ‘explanations should not be too technical’, ‘explanations are necessary but not sufficient’, and more \cite{braun2016using}. These codes were then organized into themes based on the characteristic of the explanation they referred to– who receives and creates an explanation?; what does an explanation explain?; how is an explanation developed and shared?; and what
are the goals and impacts of an explanation? These themes inform the structure of this chapter. I also describe the challenges in designing effective explanations. 

Next, I identified relationships within these themes through the process of memoing \cite{bernard2017research}.  This process revealed patterns that described the qualities inherent to effective explanations. I term explanations embodying these qualities ‘Good Enough Explanations’. I borrow the term ‘good enough’ from other domains where it has been demonstrated to be a useful concept to help navigate epistemic and actionable imperfections. Here, I aim to describe what ‘good enough’ could mean for explanations that remain imperfect \cite{sloane2023introducing}.    

\subsection{Methodological Considerations and Limitations}  

This was a qualitative and interpretive research study and doesn’t aim to be reproducible. I conducted the interviews and analyzed the data. I was familiar with the data and its contexts in ways that allowed rich interpretive analysis.  

The interpretive and exploratory approach also motivated my selection of participants where my goal was to learn more about public explanations of civic AI from a wide range of stakeholders. I aimed to recruit participants with diverse experiences and relationships with predictive policing systems across their areas of participation and expertise. In this chapter, I do not report on demographic information about the participants, because this was an initial, exploratory study that focused on social roles emerging around predictive technologies, rather than personal experiences. In later work, I plan to explore the importance of gender, race, age, and other demographic distinctions in the creation of explanations for civic AI. This study does not include quantitative evidence and as such I do not present the perspectives of an evenly distributed sample of participants. Instead, this study serves as a starting point to investigate public explanations of civic AI systems and I propose concepts that can be useful for my fellow researchers to build on, nuance, or challenge. 
