Existing transparency and explainability methods have made noteworthy progress in identifying and communicating the workings of and harms associated with predictive systems. I provided a detailed account of such methods in \Cref{ch1:intro}. However, more recently, scholars have questioned their effectiveness. There is limited work focusing specifically on public explanations of civic systems through the lens of stakeholder-centered qualitative methods.   

In this section, I present an overview of the limits of approaches aimed at promoting the explainability and transparency of algorithmic systems as discussed by the broader Responsible AI literature. This is followed by a summary of few but growing works that focus specifically on public transparency and participation. The scholarship discussed is not an exhaustive, but a representative sample of works relevant to this research.  

\subsection{Limits of Current XAI Approaches }

\subsubsection{Who is the explanation for?}
Early research on explainability focused on designing explanations for internal stakeholders who perform model debugging and improvement \cite{bhatt2020explainable}. More recently, there has been an increase in efforts to widen the scope of XAI and design user-centered explanations \cite{miller2019explanation, vaughan2021human}. However, despite their widespread and note-worthy contributions, these approaches remain limited. They tend to focus on individuals as indistinguishable consumers of explanations assuming similar needs and knowledges, disregarding the diversity and dynamic nature of explanation needs of various social groups \cite{zhou2020different}. The objectivity and seamlessness of current explanations discount the role of pluralism in the design of explanations \cite{ehsan2022social, metcalf2022relationship}. They consider explanations to be universal and one-size-fits-all artifacts \cite{dodge2019explaining}. In doing so, they fail to conceptualize explanations as social artifacts \cite{miller2019explanation}. Situated considerations are not reducible to optimizing for efficient and accurate transfer of information \cite{sloane2023introducing}.

Additionally, current efforts focus on individuals as consumers of information outside of a social group, community \cite{eyert2023rethinking} or the broader public. Focus on individuals prevents social groups from taking collective action and allows AI developers to escape the consequences of any misconduct \cite{henriksen2021situated}.   

\subsubsection{What is being explained?}

For black-box algorithms, post-hoc explanations are being designed where another human-interpretable model imitates a more complex model, in order to understand how an algorithm makes its predictions \cite{guidotti2018survey}. The majority of these efforts focus on making transparent the technical aspects of AI systems \cite{corbett2023interrogating}. Beyond making models interpretable, they attempt to make known other technical aspects such as datasets, training algorithms, source code, and performance metrics \cite{vaughan2021human}. However, merely technical transparency has been considered insufficient to support effective decision-making as it disregards the contextual factors surrounding any algorithmic decision \cite{ehsan2021expanding}. Even when technical transparency is provided, end users may not be equipped to connect these materials to the effects of the AI system on their communities \cite{ehsan2021expanding, bhatt2020machine}. Another aspect of transparency, discussed by Eypert and Lopez, is its scope. Transparency efforts do not consider elements of an AI system that are needed for democratic oversight. For instance, they do not account for media discourse surrounding the use of an AI system that may create harmful or incorrect expectations \cite{eyert2023rethinking}.  

\subsubsection{How is an explanation shared?}
 
In this work, I do not focus on interpretability techniques such as post-hoc explanations. Instead, the focus is on approaches to engage users in explanations. Current explanation methods tend to be passive and isolated \cite{corbett2023interrogating}. They only allow for a uni-directional flow of information from the makers of technology to the consumers of technology, experts to non-experts, engineers to users \cite{corbett2023interrogating, eyert2023rethinking}. Additionally, they tend to be overly detailed and technically comprehensive. However, explaining an AI system in its entirety and merely making visible the `black box' does not guarantee that a user understands the system they see \cite{liao2021human}. An overload of information about predictive tools offered quickly can not only be unnecessary but can overwhelm citizens \cite{eiband2018bringing}. At other times, despite the system's goal to be transparent, the information may be difficult for users to find or the information may not be relevant, clear, or consistent \cite{robertson2021modeling}.

\subsubsection{Why do we need explanations?}

One of the primary goals of user-centered explainability has been to promote trust in predictive systems \cite{grimmelikhuijsen2023explaining, kizilcec2016much, toreini2020relationship}. Cultivating trust in users has been considered useful for several reasons such as increasing adoption, taking advantage of the full range of AI's potential including an increase in productivity, or is considered intrinsically valuable \cite{knowles2022many}. However, increased transparency can result in users over-trusting predictive systems and placing more value in a prediction than it deserves \cite{banovic2023being}. XAI efforts may also mislead users into thinking that they have more control over predictive systems than they actually do \cite{lakkaraju2020fool}, thereby shielding developers and the algorithms from any blame \cite{lima2022conflict}. More fundamentally, scholars also remind us that trust is only desirable if it is deserved \cite{jacovi2021formalizing}. `Warranted distrust' can be more productive when we engage with imperfect AI. It can help caution users about the limited capabilities of predictive systems \cite{harrison2001trust}.  Recent scholarship has highlighted the importance of problematizing public trust \cite{gupta2023mapping} and developing critical thinking \cite{danry2023don}. 
 
The limits of XAI work summarized in this section serve as my motivation to ask: How can we address these XAI limits and design effective public explanations? Below, I present related work that provides us a foundation of research to build upon.

 \subsection{Public Engagement With Civic AI Systems }

Current XAI scholarship, including user-centered XAI, has given little attention to public-centered explanations of civic AI. However, slowly but increasingly, scholars are coming up with toolkits, frameworks, and guidelines to meaningfully engage the public in civic AI. The Algorithmic Equity Toolkit (AEKit) allows the public to define AI, determine if a tool qualifies as an AI, learn about AI's constituent parts, and develop a vocabulary of probes to ask critical questions about AI \cite{krafft2021action}. The Model Card Authoring toolkit supports community members in collectively deliberating over the design of models, the opportunities they present and their limitations, and how can they serve the needs of their specific communities \cite{shen2022model}. WeBuildAI supports communities in coming together in a participatory manner to define AI policy \cite{lee2019webuildai}. 

Frameworks and guidelines for public engagement have also been proposed: Michele Gilman published a report with Data and Society that identifies eight principles for meaningful public participation in AI governance which includes methodological and motivational components \cite{gilman2023democratizing}. A similar report published by the Data Justice Lab discusses how the construction of mini-publics, citizens assemblies, citizens juries, and participatory budgeting, among other community groups, can support civic participation in relation to algorithmic decision-making \cite{warne2021advancing}. Anna Colom of the Ada Lovelace Institute also provides starting points for meaningful public participation such as by designing systemic processes for continuous public participation in AI decision-making \cite{adalove}. 

This study builds on the scholarship above in the following ways: (1) it focuses explicitly on explaining \emph{civic} AI systems, (2) it directly engages with diverse stakeholders to conceptualize our understanding of effective civic AI explanations, and (3) it organizes our qualitative findings and existing literature to term `good enough explanations' that can serve as a useful tool to inform the design of public explanations. 


