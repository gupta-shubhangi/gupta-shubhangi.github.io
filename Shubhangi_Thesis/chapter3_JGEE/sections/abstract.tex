I begin this dissertation by asking: What qualities underlie effective public explanations of civic predictive systems (RQ1)? In this chapter, I report on interviews with people who me and my collaborators see as stakeholders in civic AI, such as academics, journalists and leaders in civic organizations and neighborhood associations, all of whom seek pragmatic explanations for the predictive technologies that are poised to change the communities in which they work. These interviews shed light on the qualities and concepts that such stakeholders are looking for in effective explanations of civic predictive systems. As mentioned in \Cref{ch1:intro}, this research draws on place-based predictive policing as a case study. Participants in this study identify the following four questions as imperative for creating effective public explanations: (1) who receives and creates an explanation; (2) how is an explanation developed and shared; (3) what does an explanation explain; and (4) what are the goals and impacts of an explanation. In following, I articulate the need for situated, systemic, continuous and partial, and actionable public explanations of AI for the civic realm. I call these `good enough explanations'. 