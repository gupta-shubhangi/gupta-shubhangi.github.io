\subsection{Exiting methods to explain AI systems as socio-technical assemblages}

Majority of existing efforts to explain AI focus on technical components such as algorithmic features and their weights, training datasets, source code, and performance metrics \cite{vaughan2021human}. Recently, the XAI community has called for moving beyond technical transparency and considering other socio-technical elements impacting AI decision-making. Few but growing frameworks and concepts are being proposed. Ehsan et al. introduce social transparency, which includes making transparent the technological context, decision-making context, and organizational context surrounding algorithms \cite{ehsan2021expanding}. Similarly, Kroeger et al. account for user literacy and propose ‘social explainability’ as a concept that explains not just the AI system but the AI ecosystem \cite{kroeger2022social}. They argue that insight into the institutional systems that AI becomes a part of can help users trust the institutions deploying AI even if they do not understand how an AI tool works \cite{kroeger2022social}. Cobbe et al. argue that current XAI methods provide insufficient information to regulate AI systems and propose ‘reviewability’ as a framework for more meaningful transparency across the entire AI lifecycle \cite{cobbe2021reviewable}. Geiger et al. use a “sociotechnical systems approach” and propose reimagining ‘audits’ as tools that don’t settle ‘matters of fact’ but instead that open up investigation into ‘matters of concern’ \cite{geiger2024making}. Such a change in perspective, they urge, will prompt us to ask questions about algorithmic systems in relation to the organizations that deploy them. Yu-Shan Tseng specifically focuses on Urban AI and proposes ‘assemblage thinking’ as a methodology to study the relations between algorithms and the cities they are situated in \cite{tseng2023assemblage}. They present a case study of vTaiwan, an open-source algorithmic platform, as situated in complex urban assemblages. They describe in detail its political and collaborative origins and how it creates communities for democratic engagement. 

Community-centered methods have also been proposed or employed to study AI systems as socio-technical assemblages. Rob Kitchen echoes the need to study algorithms as situated in broader socio-technical assemblages. He recommends conducting interviews and ethnographies of particular AI systems to shed light on the principles they adhere to \cite{kitchin2019thinking}. Seaver proposes using ethnographic methods, living with-in algorithmic systems, to know not just how algorithms work, but how they come to be \cite{seaver2019knowing}. Stop LAPD Spying Coalition worked with an activist organization called the Free Radicals and investigated the use of Predpol in Skid Row, a community in downtown Los Angeles. They proposed a framework called the ‘Algorithmic Ecology’ that maps, visualizes, and communicates the relationships of power that surround any algorithmic technology \cite{stoplapd}. 

Researchers and artists have attempted to visualize the socio-technical nature of AI systems for better user understanding. Kate Crawford and Vladan Joler designed a visualization titled ‘Anatomy of AI’ that traces AI throughout its lifecycle and makes transparent the resources that help develop an AI system \cite{crawford2018anatomy}.  

These works motivate the need to understand AI systems as complex and entangled networks of social, historical, cultural, and technical components. I add to this rich foundation of methods to explain AI as socio-technical assemblages.  


\subsection{Explaining AI systems by local publics }

Another shift in XAI efforts has been guided by the desire to move beyond merely ‘experts’ providing explanations to ‘non-experts’. Experts, from their own standpoints, are positioned well to explain specific parts of AI systems. However, scholars are increasingly recognizing the need to engage with the public to understand the effects of algorithms. Eyert and Lopez’s ‘transparency as a communicative constellation’ calls for multi-directional transparency where tech experts don’t just teach, but listen and learn from publics \cite{eyert2023rethinking}. In a similar vein, Nicenboim et al. call for co-creating an understanding of AI systems with both users and artificial agents \cite{nicenboim2022explanations}. They argue that explanations and methods of understanding offered by citizens are no less legitimate than the explanations offered by experts. Engaging with external actors and diverse publics can help identify relevant algorithmic concerns and needs \cite{nicenboim2022explanations}. Sloane echoes these values and presents participation by a more diverse set of actors, especially in ways that are “unscripted, unpredictable, and are beyond the grasps of bureaucratic control”, as a technique to ‘unblackbox’ AI systems \cite{sloane2024controversies}. Corbett and Denton join fellow researchers and respond to the technocentric turn that the XAI communities have taken, by making calls for ‘bringing people to transparency’ \cite{corbett2023interrogating}. ‘Participatory AI’, as defined broadly, provides various degrees of power \cite{corbett2023power} to the public to inform the design, use, assessment \cite{blair2019exploring}, advocacy \cite{krafft2021action}, and governance \cite{seger2023democratising} of AI systems. 

Growing, but still nascent work, has involved users as experts who can explain AI systems. Barnett and Diakopoulos propose crowdsourcing as an effective method to anticipate societal algorithmic harms \cite{barnett2022crowdsourcing}. They utilize the diversity of Amazon Mechanical Turk workers to describe the impact domains of algorithms used by the U.S. government. Marian presents a brilliant case study where parent groups and researchers come together to understand the (New York City) NYC School Matching Algorithm \cite{marian2023algorithmic}. The authors crowdsourced information about the lottery numbers that students received and the schools they were matched to understand the role lottery numbers play in the matching algorithm and identify cut-offs for various schools. Their results were made available publicly to help keep student applicants in the following academic years informed about how the system works. Other `citizen-science' projects have attempted to gather distributed information about technological systems. The `ban the scan' project by Amnesty International mobilized volunteers to share information about the presence of surveillance cameras on the streets of NYC and Hyderabad \cite{banthescan}. By doing so, they were able to explain the omnipresence of discriminatory facial recognition systems and call for a policies that ban these tools. These examples of crowd sourcing are inspiring starting points. However, they are still driven by researchers and not publics. This does not allow publics to draw on their multi-faceted expertise. Rather they provide information as requested by the researchers. They involve publics only to the extent that they can provide experiential data points without considering them experts of their own elements in the broader AI system. 

I build on these works described above in three ways: (1) I provide publics with an open platform to explain distributed parts of an AI systems based on their expertise, (2) I bring the expertise of diverse publics together instead of focusing on one aspect explained by one stakeholder type and , (3) I focus on nuanced grounded explanations instead of factual data points.   
