AI systems exist as socio-technical assemblages of histories, cultures, institutions, and practices. AI tools affect and are affected by interactions with these broader networks. How then can we understand these networks and their underlying mechanisms to identify, assess, and regulate AI related social harms? This chapter contributes to existing literature on studying AI systems as socio-technical assemblages by empirically investigating the role that actors inhabiting parts of an AI system can play in partially explaining them. I find that partial explaining by diverse publics can collectively advance our understandings of AI as socio-technical assemblages opening up novel critical questions about them, and identify gaps in current explanations thereby creating explanation needs that we can attempt to collectively address. 

Recently, scholars are suggesting the possibility of involving publics in explaining AI systems. However, there has been very limited research that has attempted to do so, or has studied how to do so. Some research has shown the potential of crowdsourced data via citizen science projects in explaining parts of an AI system. This limited work has still been highly impactful in calling for more transparency, mobilizing for regulation, and identifying AI related impacts \cite{marian2023algorithmic, banthescan}. Through this work, I demonstrate how these practices of explaining by publics can grow and the role XAI researchers can play in doing that.

Firstly, in this work, I followed the lead of publics' when generating explanations. I provided them with an open platform to draw on their unique and diverse expertise and explain parts of the system they have observed, experienced, or engaged with. This, as I note in my findings, generated explanations about elements I had not considered in the workshop design as well pluralistic conceptualizations of civic futures \cite{howell2021calling}. Such explanations highlighted parts of the AI system that actively shape how publics' perceive, relate to, or experience AI. These parts can now be further explored and researched to understand how they work to shape AI systems and their effects. 

Secondly, I encouraged publics to explain AI systems in grounded and nuanced ways. I designed spaces and tools to help them reflect on their expertise as both citizens and workers in society to identify and explain parts of an AI system. Such grounded explanations helped us understand the relation between the technical and the social, political, and historical elements of AI, avoiding the need for abstraction or assumption. The nuance in the explanations I report are absent in existing XAI processes, keeping them disconnected from on-the-ground lived realities of civic AI. 

What role can the XAI community play in supporting the generation of partial explanations by local publics? Below, I reflect on my research method along with supporting examples to describe in more detail the role XAI researchers can play in co-creating effective partial explanations with local publics: 

\subsection{Gather Partial Local Explanations} 

As I find in this chapter, local publics have the ability to explain parts of an AI systems through their role as actors in this system. I begin by urging the XAI community to gather such partial explanations instead of relying solely on tech experts or insiders to generate explanations. This will not only help overcome the epistemic challenges presented by black box algorithms or the access challenges due to lack of cooperation by technology makers, it would also help us understand how AI systems work as socio-technical assemblages that interact with, affect, and can possibly harm real world spaces. In seeking such partial explanations, there remain important factors to consider including: (1) who is involved in partially explaining AI systems and how their knowledges are privileged or discounted, and (2) how can tools, methods, and protocols support publics  in trusting their expertise and explaining AI systems as they relate to them. In this work, I gathered partial explanations from diverse groups who I believed had a stake in the working of predictive policing. My efforts were limited by (1) who has the social capital to organize in ways that make them approachable, and (2) my ability to form relations with social groups. I used participatory methods, grounded in the places people know and consider their own to encourage participants in sharing their knowledges.  

\subsection{Organize Partial Local Explanations}

Next, I highlight the need to organize and formalize these partial explanations. While partial explanations in themselves help in the development of nuanced understandings of AI, when such explanations come together, they can help identify patterns and draw insights into how an AI model works in relation to other socio-technical components more generally and related potential harms. However, as Loukissas, reminds us, ‘all data are local’. So are all explanations. Like data, explanations too are created by specific people, using specific tools, and for specific goals. The challenge is to acknowledge and preserve the locality of explanations even as we attempt to organize them in effective ways. In this chapter, I attempted to organize explanations (1) locally, by visualizing effects on a shared map, and (2) across workshops with the organizers acting as threads bringing explanations from one workshop to the other and finally together in this writing. There is more work to be done. I plan to organize the collection of explanations spatially through the use of an interactive map to eventually identify how predictive policing may affect spaces disparately. 


\subsection{Continuous Partial Local Explanations} 

Lastly, I discuss the need to consider how to place these explanations in systems that allow continued development and iteration of explanations. The development and organization of partial explanations are not a one-time effort. In the ever-changing world of AI, the explanations publics choose to, or have the ability to offer will also continue to evolve. Such developments will require redevelopment and reorganization of partial explanations. To promote continuous development of explanations, these processes of explaining need to be placed in systems that bring people together, time and again, around growing capacities of AI, to engage in continued and long-lasting explaining . Unfortunately, the workshops were not part of an official or robust system that had the capacity to be long-lasting. Yet, the work, done as part of a research project, with a fixed timeline and funds, was able to form a small system of five workshops over a period of a year for the continuous development and organization of partial explanations. In the process, I identified other sites that may be useful for continued engagement such as the (Neighborhood Planning Unit) NPU University \cite{npu_university} that invites people to learn about civic processes every spring and fall. 

\subsection{Limits}

I acknowledge that such partial explanations may not always be concrete, complete, or accurate, nor do they need to be, in order to support critical engagement with AI systems. In saying so, I am following the lead of Gabrys et al. \cite{gabrys2018just} who argue that citizen data may not be complete or accurate but is ‘just good enough’ to create a shared space for discussion. Similarly, partial explanations offered by local publics are ‘just good enough’ to present concerns and launch an inquiry into the effects of AI on society. 

Ultimately, I call for the broader HCI and XAI communities to design spaces for partial explaining. I elaborate on this in the next chapter. I hope that such spaces can provide systemic and long-lasting ways to support democratic and critical engagements of local publics with AI systems and their underlying power structures \cite{geiger2024making}. I would like to note that in this chapter, I do not play the role of XAI developers trying to partially explain place-based predictive policing. Instead, I serve as a design researcher identifying the role of local publics in the design of AI explanations. My goal then is not to explain, but to study explanations. 
