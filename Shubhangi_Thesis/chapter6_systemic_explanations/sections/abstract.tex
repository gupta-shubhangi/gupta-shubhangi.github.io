In the last chapter, I discussed how explanation needs are situated in diverse explanation contexts. These explanation needs inquired not just about the predictive tool but also the complex systems that such tools become a part of. As such, this chapter asks: \textit{How can we move towards a better understanding of the expansive and complex socio-technical assemblages underlying AI systems?} 

AI systems emerge out of broad networks of materials, relations, cultures, institutions, and histories that may affect societies in unjust and harmful ways. In this chapter, I argue that local publics can partially explain how algorithms interact with society to affect local contexts. I report on our efforts to engage with these diverse partial explanations through the participatory mapping workshops detailed in \Cref{ch3:methods}. I find that partial explaining can (1) collectively advance our understandings of AI systems as socio-technical assemblages, opening up novel critical questions about these systems, and (2) identify gaps in current algorithmic explanations thereby creating explanation needs that we can attempt to collectively address. I call for the creation of spaces and systems where such engagements of partial explaining and knowing can happen. 