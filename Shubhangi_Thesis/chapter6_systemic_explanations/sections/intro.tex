Today, our efforts to explain AI systems are limited to opening the ‘black box’ and learning about the data, variables, algorithms, and models that it builds on. As models become more complex, are constantly changing and evolving, and are contingent on countless unknown data and their categorizations, this black box becomes tougher to look into \cite{nicenboim2022explanations, seaver2019knowing}.

Additionally, existing pursuits towards transparency are overwhelmingly technical and disregard how algorithms interact with broader networks of materials, relations, cultures, institutions, and histories to affect societies in unjust and harmful ways. A meaningful examination of an AI system requires us to engage with the socio-technical assemblages in which it is placed \cite{kitchin2019thinking}. For example, understanding predictive policing systems as socio-technical assemblages would include learning about the spaces they categorize as crime hotspots and their characteristics, their effect on relations between police and communities, and the policies, laws, and protocols they engage with. In what follows, I use the term `AI systems' to refer to these broad and complex socio-technical assemblages. 

There is a need to (1) overcome the epistemic barriers presented by opaque ‘black-boxed’ algorithms, and (2) situate algorithmic systems in broader networks of spaces and environments that affect and are affected by the algorithm \cite{tseng2023assemblage, gupta2022rethinking}. I follow the lead of researchers who address these challenges by proposing to ‘look across’ the black box, rather than ‘look inside’ them \cite{ananny2018seeing, nicenboim2022explanations}. Understanding algorithms as part of wider urban assemblages that continuously modify and evolve can surpass the dependence of knowing AI systems merely by opening the black box and would allow us to understand AI systems more holistically in relation to the socio-technical networks they become a part of. To that end, in this chapter, I ask— How can we move towards a better understanding of the expansive and complex networks underlying AI systems? 

In their entirety, AI systems always remain unknowable \cite{crawford2021atlas}. As with all knowledge, they can only ever be known from specific standpoints and in specific settings \cite{harding2013rethinking}. Publics, from their own standpoints, are experts of their own domains \cite{suchman1987plans}, and possess knowledges that can add to, complement, or even challenge the perspectives of experts \cite{nicenboim2022explanations}. I empirically explore the role local publics can play in explaining elements of the broader AI system from their unique and grounded standpoints.   

To do so, I draw on the participatory mapping workshops I conducted that mediate collaborative processes of explaining and questioning civic AI systems. As detailed in previous chapters, these workshops engaged with diverse publics including police reform groups, urban planners, neighborhood leaders, funding agencies, and teachers. In the process of conducting these workshops, I find that local publics are positioned well to partially explain how algorithms interact with society to affect local contexts. Locals directly engage with components of AI systems outside of the ‘black box’ and can make known, amongst other aspects, the environments AI tools are deployed in, the cultures and norms they invade, and the lived experiences of the problems they attempt to address. As such, local publics possess partial explanations of AI systems that can come together to raise meaningful and grounded questions about the effects of AI systems. 

Ultimately, I argue that bringing partial local explanations together can help us collectively advance our understanding of AI systems as socio-technical assemblages opening up novel critical questions, and help identify gaps in current explanations thereby creating explanation needs that we can attempt to address. I call for the Explainable AI (XAI) community to design spaces that can invite local publics to collectively uncover nuanced networks underlying complex AI systems. 

In what follows, I summarize existing approaches that explain AI systems as socio-technical assemblages and if and how publics have been invited to participate in the design and governance of AI systems in the `background' section. Next, I briefly summarize the data analysis methods for this chapter in the `data analysis' section. I report on the explanations publics provided in the `findings' sections. I end by discussing the role of XAI researchers in designing systems for partial explaining and knowing. 