\chapter*{Afterward}

Following the completion of my research, I have reflected on my choice to focus on place-based predictive policing as a case study. I believe my choice of this case affected my research in fundamental ways. Here I document what motivated my selection of the case, how the case supported my work, the challenges I faced, and my learnings. My hope is that these reflections can help other students and researchers think about how to approach such selection of cases as they study the design of public centered explanations or technologies more broadly. 

Place-based predictive policing was unique as a case study for studying effective explanations due to several reasons: 

\textbf{Availability and access to information.} Place-based predictive policing systems have been in development and use for over a decade now. They have been thoroughly studied by activists and academics who have attempted to understand the workings and effects of such systems. The breadth of work done on such systems led me to believe that it may be logistically possible to access information about these systems and therefore explain these systems to diverse publics. 

However, I realized in my research that despite the long-standing existence and awareness about the harms of these tools, how these tools function or become part of civic processes still remains unknown. These black-boxed systems are being protected by black boxed institutions such as by the technologists making these tools and policing institutions using these tools. Stakeholders in our study suggested that such protection is intentional. They explained that the more information that is available about these tools for public oversight, the more these organizations are exposed to backlash related to legal and human rights violations. The opaqueness of these systems was also made apparent by the hundreds of Freedom of Information Act (FOIA) requests filed by journalists that were declined. I, myself, attempted to fill out requests for information about the use of these tools in Atlanta. I used existing FOIA requests as samples and requested information about the AI tools used by the Atlanta Police Department and related white papers, audits, data, or handbooks. My request was quickly declined. This was surprising to me as similar requests, such as the ones I used as samples,  had been fulfilled in other states or contexts. In fact, a small part of the information I requested was already shared as part of another public FOIA request. The decision to comply to my specific request was ultimately at the behest of the responding officer. And with little legal knowledge about how to navigate the FOIA process or challenge the decision, I was unable to do much about their choice to decline my request. 

Such opaqueness has also discouraged further investigation of these tools. A journalist I spoke with during my work explained how she has now stopped writing about predictive policing systems as they are too difficult to access. Another stakeholder, a police officer that I reached out to, mentioned the use of predictive policing tools in Atlanta but was unable to share anything else about how it worked or the protocols surrounding the tool. The officer also did not respond to repeated requests for further conversations. This was not surprising as other police officers and agencies I had reached out to in the past had never commented on the tool. I am not sure if they themselves did not know about these tools or were unwilling to talk about these tools due to systemic fear, lack of incentive/time, or loyalty towards their departments and associated practices. I believe investigations of such systemic barriers to transparency are highly important to help make civic AI systems known to publics.

Ultimately, while it was difficult to access these systems, my investigation of this specific case helped me understand how civic systems work (or do not work) to limit public access to civic AI tools. 

However in my particular case, due to limited access to information about these systems, I employed speculative personation as an activity to collectively develop good enough explanations. One could argue that the explanations about AI tools that emerged out of this activity are in fact \textit{speculations }themselves and may not be grounded in hard facts. Even as I, the moderator, attempted to ground speculations in algorithmic components through the prompts in the workshop protocol, there was no effective way to identify whether the speculations were in fact the ‘reality’. Yet, the process proved useful in three ways: (1) it helped participants develop a critical understanding of how they \textit{expect }AI systems to work even if they did not know how they \textit{do }work; (2) it allowed participants to \textit{explain }the socio-technical assemblages surrounding AI tools in good enough ways. These explanations, unlike the speculations that they may have emerged from, were grounded in local experiences and knowledges, and (3) it helped identify, not what is already known, but what we need to know about the technical or socio-political elements of an AI system to eventually develop a good enough understanding. Ultimately, speculative personation acted merely as a means, a method of doing and imagining, to eventually develop good enough understandings. 

\textbf{Investigating spatial effects.} Place-based predictive policing systems provided a unique opportunity to investigate the \textit{spatial }effects of civic AI tools, that remain understudied. As I discuss in my Introduction, several other civic AI tools affect spaces, through spatial variables like zip codes or proxy variables like income or race. However, place-based tools center their very predictions in `spaces', attempting to dehumanize spaces as inanimate objects that affect crime patterns in generalizable ways. Place-based predictive systems, therefore, provided a distinctly spatial premise to investigate the spatial workings and effects of AI. Some other place-based tools like disaster relief management tools or market analysis tools may serve as similar cases to study in future work. 

\textbf{Relevance for the site of study. }Place-based predictive \textit{policing }systems served as a relevant case for my site of study: Atlanta, GA. Atlanta has a history of segregating communities using urban infrastructure such as highways and railways. More recently, an Atlanta based policing project, called the `cop city' by its critics, is being criticized for promoting oppressive policing tactics. Atlanta is also known as the ‘most surveilled’ city in the United States. Additionally, many of Atlanta’s technology-based policing projects are funded by the Atlanta Police Foundation, a private organization that does \textbf{not }need to comply to public requests for information or public oversight and assessment. As such, I expected the case of place-based predictive policing would hold importance for the citizens of Atlanta and would encourage them to question, understand, and explain these AI tools. My work received attention and praise from many non-profits and social groups who related well to the case, realized the importance, and volunteered their time to participate enthusiastically. While the workshops were grounded in this particular case, participants often brought other cases to the discussion table including other policing tech (facial recognition or license plate readers) and other everyday use tools (ChatGPT, ads, finger print readers). The workshops allowed the participants to deviate to cases most relevant to them and consequent workshops intentionally engaged with tools that participants were interested in. 

In summary, while my selection of this case presented challenges, such as accessing information about these tools, it still allowed me to engage with and study the civic systems protecting the tools. The lack of information on such tools combined with the high impact they have on citizens, further motivates the need for continued investigation of these tools. My case also aligned with my research interests and site and (1) allowed me to study \textit{spatial }AI and (2) served as a case that was important and relevant for my participants in Atlanta. When selecting a future case, I expect to face similar challenges in other civic domains, sometimes more than others, depending on how transparent a domain is and the socio-political climate of trust or distrust that surrounds it. In my future work, I also hope to diversify cases and focus on tools that citizens may directly engage with. Drawing on my findings that demonstrate publics’ ability to explain AI tools, I would be interested in how explanations may emerge out of publics’ conscious and direct engagement with an AI system. 

I hope the XAI community draws on my findings from this work and nuances them by studying diverse cases in relation to publics’ diverse settings. 
