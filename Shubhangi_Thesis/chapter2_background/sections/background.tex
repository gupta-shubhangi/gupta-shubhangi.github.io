In this chapter, I provide a summary of the fields and works that this dissertation is in conversation with. I begin by describing works that document the impacts of public safety tools on urban cities and motivate their use as a case study for this work. Next, I clarify the use of the term 'explanation' in this dissertation and how it relates to adjacent fields such as 'AI Literacy' and 'AI Transparency'.  Next, I summarize existing XAI techniques and their limits. Lastly, I describe the use of participatory design, specifically participatory mapping in studying complex socio-technical phenomenon, My motivations for use of participatory mapping is further detailed in the methods section.  

\section{Public Safety Algorithms and Harms }  

My dissertation started with a critical analysis of a renowned safe walking app, primarily deployed in India, called `Safetipin' \cite{safetipin}. Safetipin recommends `safe' paths to users from an origin to a destination by calculating `safety scores' for various paths. These safety scores are calculated by aggregating crowdsourced `safety data' such as the amount of lighting, or presence of security officers, in various locations in a city. I demonstrate Safetipin’s capacity to (1) restrict women’s movement to computationally calculated `safe' neighborhoods and (2) reinforce caste and religion-based segregation in India \cite{gupta2022rethinking}. By disregarding the prejudice about vulnerable neighborhoods that governs the `feeling of safety' of its users who contribute to the crowdsourced data, it fails to situate itself in the broader historical politics of safety in the city \cite{kern2021feminist} that continues to marginalize people of lower socioeconomic status and minority religions. Nonetheless, the app and its underlying information infrastructures that promote segregation, have been enthusiastically accepted and celebrated \cite{safetipin_award1, safetipin_award2}. This work presented a dire need to understand the impact of spatially distributed data inputs and aggregations on the city and its people. Ultimately, it motivated the need to explain how emerging civic geospatial technologies, especially in the realm of public safety, organize cities and their impact on spatial segregation and discrimination. 

Amongst other tools for public safety exist a variety of ML algorithms that have been developed with the hope to mitigate crime and advance citizen safety in smart cities. Popular examples include— COMPAS, which predicts recidivism risk for an individual; Predpol (now Geolitica), which predicts geographic areas where crime is most likely to happen; Arnold Public Safety Assessment, which provides judges with sentencing recommendations. These algorithms, even as they aim to promote public safety in cities, tend to reinforce discrimination along the axes of race and class. Jefferson demonstrates how Predpol legitimizes the bias embedded in official crime datasets and has resulted in the over-policing of already heavily surveilled neighborhoods \cite{jefferson2018predictable}. Risk assessment tools build upon and reinforce the racist policies and infrastructures underlying carceral systems in the US. Additionally, they define `risk' at the level of an individual, disregarding how `risk' is a reflection of societal prejudice against various social groups \cite{green2020false}. In India, centralized systems and norms along with the subjectivities of individual police officers lead to historical, representational, and measurement bias in recorded crime data for the CMAPS predictive policing tool. Further, the opaque design of CMAPS allows for discrimination against immigrant colonies and minority settlements by promoting the belief that crime rises in specific neighborhoods by virtue of the above-mentioned communities living there  \cite{marda2020data}. Transparency is a much-needed feature for the effective assessment and development of public safety algorithms \cite{rudin2020age}. Given the limited potential of techniques designed to “de-bias” public safety algorithms, there is an urgent need to make the data assemblages \cite{kitchin2014towards} surrounding public safety algorithms transparent and accessible to city residents and governmental bodies \cite{marda2020data}.   

This dissertation aims to study the concept of effective public explanations,  by specifically focusing on place-based predictive policing as a case study. Place-based predictive policing is a method that aims to support the efficient distribution of police resources in a city. Typically, the method utilizes historic crime data such as arrest reports or calls for service requests integrated with other social and environmental data to predict the location and time of future crimes. In the past decade, academics and activists have heavily scrutinized the use of predictive policing \cite{o2017weapons}. The tool has been shown to reproduce existing geographic and social biases embedded in historically discriminatory crime data \cite{jefferson2018predictable,akpinar2021effect}. A recent study conducted by the Markup has found the accuracy of this tool to be less than half a percent \cite{predpolbad}. Many others have discussed the incompleteness of data \cite{kirkpatrick2017s}, the inability to validate results \cite{demortain2017evaluating}, and the proliferation of positive feedback loops \cite{o2017weapons}. Such thorough investigation of this predictive tool makes it an ideal case to ground our discussions of effective public explanations. 


\section{Existing XAI and AI Transparency Methods} \label{XAImethods}  

Early research on Explainable AI focused majorly on technical explainablity and transparency. For black-boxed algorithms, post-hoc explanations where another human-interpretable model imitates the practices of a complex model are being designed to understand how an algorithm makes its predictions. Guidotti et al. \cite{guidotti2018survey} categorize post-hoc explanations into these categories: (1) Global Model Explanations where a simpler interpretable model is trained on the same data to approximate the working of the primary more complex model, (2) Outcome Explanations where the focus is on explaining one outcome or instance of prediction by providing the weight of features that contributed to it \cite{schuff2022human} or examples of inputs that would lead to a similar prediction. (3) Counterfactual Explanations where the goal is to identify what should be changed in the inputs to get a different prediction \cite{shang2022not}. Beyond making models interpretable, XAI efforts attempt to make known other aspects of the machine learning model such as datasets, training algorithms, source code, and performance metrics \cite{vaughan2020human}. Several model \cite{mitchell2019model} and data documentation \cite{anik2021data, bender2018data, gebru2021datasheets, holland2020dataset} frameworks, have been designed to explain AI to experts \cite{dhanorkar2021needs}. Frameworks such as `CrowdWorkSheets' support standardized documentation of decisions when annotating datasets in a crowdsourced manner \cite{diaz2022crowdworksheets}. `Data Cards' provide summaries of datasets for various stakeholders \cite{pushkarna2022data}. To promote transparency of models, existing work proposes tools such as `Model Cards' that aim to provide details about a model related to its working, use cases, and evaluation \cite{mitchell2019model} and `Factsheets' that provide an overview of facts about specific models across the AI lifecycle \cite{richards2021human}. There is also growing work in designing open-source toolkits to identify and assess algorithmic harms and biases \cite{bellamy2018ai, wexler2019if, bird2020fairlearn}. AI Fairness 360 (AIF360) \cite{bellamy2018ai} and Fairlearn \cite{bird2020fairlearn} are tools that aim to help practitioners understand `bias' metrics and allow them to detect algorithmic biases. These tools also help mitigate said biases by providing a variety of mitigation algorithms. Another tool called ``What-If'' uses visualizations to help users and practitioners investigate how a model will perform in hypothetical scenarios created by changes in data points \cite{wexler2019if}.

However, more recently, the XAI community has presented the need to make AI processes visible to the general public to build trust in the artificially intelligent systems that guide their lives \cite{knowles2021sanction}. As such, there is a growing focus on developing methods and frameworks that provide user-centered algorithmic explanations \cite{miller2019explanation, shneiderman2020human, vaughan2020human}. Human cognitive abilities \cite{wang2019designing}, users' explanatory needs \cite{liao2020questioning, springer2020progressive}, users' situated real-world experiences \cite{devos2022toward}, users' ability to collaborate and form counter publics \cite{shen2021everyday} have been some of the guiding factors in advancing user-centered XAI research.    

Such efforts have built on diverse theories and methods: interactive explainability methods \cite{cheng2019explaining} such as Interactive Model Cards \cite{crisan2022interactive} have been proposed, theories describing human cognition patterns have been employed \cite{wang2019designing}, and example-based methods where data samples that informed a prediction are shown to users have been presented as helpful \cite{cai2019effects}. Additionally, several toolkits have been designed to promote user understanding of AI systems such as: TILT (transparency information language and toolkit) that organizes the information that transparency policies demand in structured ways for machines to read and users to consume \cite{grunewald2021tilt}, or “What-If” \cite{wexler2019if} and AIX360 toolkit \cite{arya2019one} that present visualizations and explanation algorithms respectively to help users understand how predictive systems work. Investigations into how XAI can support user assessment \cite{robertson2022understanding} and decision-making capabilities \cite{kulesza2013too} are also being conducted.

These efforts have made monumental progress in the field of Explainable AI. Yet, there remain limits. First, they are not pluralistic in nature \cite{hancox2021epistemic, ehsan2022social} and do not consider the values, surrounding social systems, existing knowledges, and beliefs \cite{kaur2022sensible} of users in the design of explanations. Second, they may focus on merely on technical transparency, disregarding the systemic factors surrounding any algorithmic decision \cite{ehsan2021expanding}. Third, they tend to be one-time explanations that consider users as passive individual consumers \cite{corbett2023interrogating}. And lastly, the goal of most explanations is to increase user trust rather than to promote critical thinking or action \cite{danry2023don}. I discuss these limits in more detail in Chapter 3 which motivate our study of what makes effective public explanations of civic AI. 

This dissertation explores the possibility of drawing on visual and participatory techniques to address some of these limits. Below, I describe some other related work that inspired this research. 


\subsection{Visual explanations for AI }

Recently, XAI researchers have presented the importance of the form of algorithmic explanations and its effect on the understanding of AI systems \cite{van2021effect}.  An emerging form to make explanations accessible \cite{schor2022making}, is the integration of visual design and XAI efforts \cite{cheng2019explaining}. In this section, I present a summary of the works that draw on visual techniques to explain algorithms.  

Integrating XAI and visual design can make customer-facing explanation interfaces more usable and readable \cite{cheng2019explaining}. The effects of static and interactive visualization techniques of white box (where a model's inner working is shown) and black box (where input and output variables' relationships are shown) explanations on user comprehension have been investigated \cite{cheng2019explaining}. It was found that white-box interactive explanations were most effective in increasing user understanding but were worse than black-box explanations in increasing user confidence in their understanding. This may be a result of the complexity and cognitive overload of white-box explanations. An understanding of human cognition and decision-making capabilities is now being used to develop frameworks for explaining algorithms \cite{wang2019designing}. Researchers have also attempted to visualize ethical frameworks in order to make them more accessible to users who may not be familiar with ethical principles and terminologies \cite{sleigh2020visualizing}. 

Share Lab has used data visualizations to represent several aspects of algorithms including algorithmic labor, invisible infrastructures that surround algorithms, and the social and political relations that inform the workings of tech companies \cite{sharelab}. Kate Crawford's work titled `Anatomy of AI' which displays several invisible aspects of labor, data, and environmental resources in relation to algorithms is another well-known example of visualizing algorithms \cite{anatomyofai}. These works draw great attention to the socio-political structures that surround the design of algorithms. 
 
Interactive visual analytics are being used to help data scientists better understand their systems through the design of tools such as Prospector \cite{krause2016interacting}, Gamut \cite{hohman2019gamut}, Visual Auditor \cite{munechika2022visual}, and more. These tools visualize algorithms for controlled assessment and evaluation.

Data Comics have been presented as a means to better report HCI and statistical analysis research studies and are being explored as visualization techniques to communicate research processes and practices in accessible and engaging manners \cite{wang2020data}. Economist Julia Schneider and Artist Lena Kadriye Ziyal designed a comic series that explains what Artificial Intelligence is, its core properties, and the risks associated with its widespread deployment \cite{datacomic}. Explaining specific design decisions underlying the functioning of algorithms and their impact on the lives of its users still remains underexplored. 

\subsection{Participatory Approaches to XAI} \label{parXAImethods}

Beyond merely catering the explanations to users, there have been few but rising calls for centering local communities in the design and development of ML algorithms \cite{brown2019toward, katell2020toward, krafft2021action, wong2020democratizing, shen2022model, lee2019webuildai}.  Shen et al. have presented a model cards toolkit that can be used by community members to deliberate on which model, amongst a variety of models, aligns with their values and interests \cite{shen2022model}. Lee et al. have developed a framework that supports community members in building policy to govern algorithms in a participatory manner \cite{lee2019webuildai}.  

Participatory methods have also been previously explored for analyzing the impacts of artificially intelligent systems at DIS \cite{blair2021participatory} and related venues \cite{reisman2018algorithmic, alvarado2018towards, krafft2021action}. Blair et al. propose to explore the potential of participatory art installations to perform a public assessment of predictive algorithms \cite{blair2021participatory}. The AI Now Institute proposed the Algorithmic Impact Assessment (AIA) framework and noted the need for affected communities and governmental bodies to be aware of how black-boxed automated decision-making systems work \cite{reisman2018algorithmic}. Alvarado et al. propose Algorithmic Experience (AX), an analytical tool that can be employed in a participatory manner to understand users' experiences of AI-driven tools such as the Facebook News Feed \cite{alvarado2018towards}. The Algorithmic Equity Toolkit (the AEKit) offers a collection of methods to increase the participation of the public in algorithmic advocacy \cite{krafft2021action}.

I build on the work described above to develop a participatory mapping toolkit that can help local community members and ML researchers reflect on the spatial effects of public safety algorithms. At the same time, my work differs from the approaches described in \Cref{XAImethods,parXAImethods} in a multitude of ways– (1) Subject: Unlike current approaches that focus on explaining input-output relationships, ML processes, or general AI impacts, my work focuses specifically on explaining the spatial effects of civic algorithms; (2) Methodology: Instead of relying on AI models or tech experts to develop AI explanations in a manner that is removed from the local experiences of community members, I use the technique of \emph{participatory mapping}, as described below, for providing grounded explanations; (3) Goals: In contrast to existing approaches to XAI that aim to build trust in users \cite{ehsan2019automated, lipton2018mythos}, the goal of my thesis is to problematize said trust and empower city inhabitants and ML practitioners to effectively evaluate ways in which AI systems organize cities.  

\section{Participatory Mapping as a Critical Practice} \label{critParticipatory}

Historically, mapmaking as a practice has been used to support the ideologies of its makers and has served as a tool of persuasion and power to advance colonization and imperialism \cite{wood1992power}. However, with the growth of participatory mapping, local communities have repurposed this historical practice to fulfill their own purposes and demand social and political justice \cite{sletto2020radical}. Techniques such as counter-mapping \cite{kollektiv2018not}, collaborative cartography \cite{bosse}, and participatory GIS \cite{elwood2006critical} aim to engage local communities in the process of map-making to represent and visualize existing systemic injustices and propose better futures  \cite{bosse}. There exist several influential examples. In 1971, the Detroit Geographic Expedition and Institute (DGEI) released a map called ``Where Commuters Run Over Black Children on the Pointes-Downtown Track''. The map was the result of a collaboration between young black adults from local neighborhoods and academics. Through the collaboration, the youth learned cutting-edge mapping techniques to transform their local knowledges into tools to demand justice \cite{d2021collects}. Another provocative example is the Anti Eviction Mapping effort, a participatory oral history project, which is a result of collaboration with local partners and people being evicted with the goal of resisting urban gentrification \cite{maharawal2018anti}.  

\emph{Participatory methods} aim to develop social and technical systems directly in collaboration with end users \cite{muller1993participatory}. \emph{Critical making} allows participants to focus on the shared processes of construction as a site to develop a conceptual understanding of critical sociotechnical issues \cite{ratto2011critical}. I plan to design a \emph{participatory mapping toolkit as a form of collaborative critical making} to help the makers and users of civic algorithms reflect on both the positive and negative spatial effects of public safety algorithms.  

Despite the opportunities presented by participatory mapping to investigate social justice issues, there remain limits. Participatory maps, even as they strive to be pluralistic, may still silence the voices of the least privileged while enhancing the perspective of a chosen elite. The activity may also become a benign way of involving community members while reserving the power and decision-making capabilities for the more technically proficient people \cite{guldi2017history}. For example, in the participatory mapping sessions I plan to host, people who are less acquainted with AI may feel uncomfortable participating in a collaborative setting. I acknowledge the limits of my methodology and will actively work to create a safe and respectful environment for all participants.   

 
\section{AI Explanations vs AI Transparency vs AI Literacy}

AI Explainability, AI Transparency, and AI Literacy may often used be used interchangeably or in similar contexts. How does this dissertation understand these terms? While a thorough discourse analysis of how these terms are used and relate to each other is beyond the scope of this dissertation, I hope to provide a brief description of how this dissertation understands these terms and their relations. 

In using the term `explanations', I am not referring to the technical explanations provided through the use of algorithmic interpretability methods (briefly described in the next section). Rather, I am attempting to expand our community’s understanding of `explanations' such that it is not limited to the technical understanding of the systems but is explaining the social, political, and economic development, use, and effects of civic predictive systems. These explanations then form a subset of techniques that can help make AI transparent \cite{corbett2023interrogating}. There may exist other techniques to make AI transparent such as documentation of data or models, making source code available to publics, auditing \cite{shen2021everyday}, or providing error rates \cite{robertson2022understanding}. 

For this work, I follow the lead of Long and Magerko and define AI literacy as the development of competencies that enable individuals to critically understand and use AI systems \cite{long2020ai}. AI Transparency and Explainability can support he development of such competencies. However, literacy, in itself, is a more fundamental capability developed at longer time frames and allows broad engagement with AI. 