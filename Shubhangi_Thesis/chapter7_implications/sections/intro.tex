Early in this dissertation, I started on a quest to conceptualize `good enough explanations' of algorithmic systems. Good enough explanations of predictive systems, I argued, were not complete or objective, but were good enough to support publics in critically engaging with civic predictive systems.  

We learned that explanations are good enough for \textit{someone} in their grounded contexts. Such contexts, as I demonstrated consist of how people know, experience, and feel about predictive tools, domains, subjects, and backdrops. Next, I learned that tech experts are not the only ones who can offer good enough explanations. Publics, who interact with the socio-technical assemblages underlying AI systems from their positions in this system can explain parts of an AI system. I proposed that XAI researchers could act as moderators who gather, organize, and continuously develop a growing collection of partial explanations. Such moderation however is not an easy task. 

Current work conceptualizes the `explanation' of algorithmic systems as a noun— static artifacts that can be exchanged between groups of people without changing form or meaning. However, any transfer of information requires a system of institutions, interfaces, and methods through which it travels. As it travels, it changes meaning or form. Oftentimes, in this process, it is recreated for diverse contexts creating new questions and knowledge gaps. Explanations, through this lens, are a verb, a process—explain\textit{ing}. \textit{How can XAI researchers moderate these processes of explaining with diverse audiences in partial and continuous ways? }

Explaining, understood as a process, requires us to consider the (1) sites where information is generated and consumed, (2) media through which they are explained and understood, and (3) interactions that mediate the processes of questioning and knowing. These considerations inform the design of situated and systemic processes of explaining.  

In this chapter, I foreground three related design decisions that I believe designers will have to grapple with as they operationalize the practice of good enough explaining: (1) designing explaining sites (2) designing explaining modalities, and (3) designing explaining interactions. Below, I reflect on my own experience of addressing these design decisions, along with challenges, lessons learned, recommendations, and related work. 

\section{Designing explaining sites}

\textit{Where does explaining happen? }Existing research rarely engages with the \emph{place} of explaining. When designing systems of explaining, XAI researchers must actively consider their sites, or their entry points \cite{burrell2009field}, into explaining. The design and selection of sites can determine who gets to participate, in what capacity, and to what ends \cite{loukissas2021open}. 

Typically, sites for explaining algorithms are the same platforms on which a user encounters the AI system. For example, the `why ad' button on an ad will explain why a user may be seeing an ad. Sometimes, the platform provides links to other sites where a user can get a more detailed explanation of how an algorithm works along with options to control them, as with the Ad centers \cite{myadcenter}. However, civic AI systems are not packaged in a platform that is easily accessible by the publics. \textit{Where} then can we explain civic AI systems to the public? Below, I report on my experiences engaging with this question, the constraints I encountered, and the lessons I learned. 

\subsection{Our approach, learning, and challenges}

Our approach involved exploring different sites and empirically investigating the opportunities they present along with constraints and limits. The sites I identified for initial exploration included: 

\begin{enumerate}
    \item Existing civic sites: NPU University \cite{npu_university} and Citizen's Police Academy \cite{police_academy}
    \item One-on-one partnerships: Civic organizations and non-profits (W1, 2, 5)
    \item Existing and scheduled training modules: Organization retreats (W4)
    \item Open call workshops \cite{xai_workshops} (W3)
    \item Neighborhood meetings
    \item Neighborhood meet-ups
\end{enumerate}

All these sites varied in what they offered for the purposes of this dissertation. Existing civic sites, like the NPU (Neighborhood Planning Unit) University \cite{npu_university} and Citizen's Police Academy \cite{police_academy} offered civic education to Atlanta residents. They were the most robust channels ensuring that I would be able to slowly, but \textit{continuously} engage with diverse publics. They also were formalized in trustworthy ways by being affiliated with the city of Atlanta and the city police department respectively, inviting increased participation from citizens. Yet, I was unable to organize workshops at these sites because of their rigid structures involving time intensive bureaucratic processes for approvals. I did receive positive feedback from them and they encouraged us to offer courses and workshops as part of their programs. I had to forego these options due to the time and logistic constraints of this PhD. One-on-one partnerships allowed us to seek out people who I considered relevant for this work. I was also able to select groups that may benefit from learning about civic predictive systems or will be able to explain parts of these systems themselves. W1, W2, and W5 were conducted through one-on-one partnerships. Open call workshops were hosted on the Georgia Tech campus and required extensive marketing to spread the word such as via flyers and emails (see Appendix A). These sites, however, cherry-picked people who had the interest and ability to travel to the Georgia Tech main campus. Many participants interested in open-call workshops preferred virtual meetings and eventually dropped out. Neighborhood committee meetings and Neighborhood residential meet-ups were another site I explored. These meet-ups involved voluntary participation and, despite the interest in this work, people rarely had the time or energy to participate at such sites. As such, I was unable to organize workshops at these sites.  

\subsection{Opportunities and challenges }

In selecting sites of engagement, I had the opportunity to seek out publics, focusing on stakeholders that have rarely, if ever, been engaged in conversations around civic AI systems. With this opportunity, I also carried the responsibility of selecting groups for engagement. I had the power to engage certain groups who may have had the capital or resources to organize in ways that made them approachable. I was the lead moderator of all workshops. As such, I was able to weave the workshops together, sharing discussions from one workshop in the next. This created a bigger more connected site and I had the opportunity to support indirect conversations and partnerships. Unlike workshops with organizations where team members generally shared similar ethos, open call workshops brought in diverse perspectives and rich conversations. Workshops that were part of existing modules or retreats saw motivated participants and reduced the burden of engagement.

However, designing these distributed sites of engagement was not an easy task. My efforts were limited by existing awareness and possible entry points into this engagement. Only people who had some insight into the effects of predictive tools were motivated to participate. As such, I was unable to reach those who were farthest away from an understanding of AI. I was also aware of the burden of engagement the workshops put on the participants, especially open call workshops that could be organized far away from publics' place of residence or work. 

\subsection{Other inspirations}

There is a scarcity of sites that allow for scalable or systemic approaches to achieve transparency \cite{sloane2023introducing}.  Some scholars studying data literacy such as McCosker et al. suggest that we must intervene in `organisational data settings', sites where data is developed and used, especially by community organizations and non-profits who work directly with communities. This, they urge, will help address any  ‘expertise lag’ that may occur when working with data \cite{mccosker2022developing}. Pallett et al. propose the development of \textit{observatories} to mediate public engagements with civic AI systems. The use of observatories will allow XAI researchers to take advantage of existing civic systems designed for public engagement. They can also be placed in a manner that allow ongoing conversations in response to the continuously growing worlds of AI. These observatories can also be designed to deliberately focus on under-represented communities and mediate conversations with diverse and distributed communities \cite{pallett2024just}. 

Current approaches that mediate community engagement with civic AI tend to do so by partnering with non-profit civic organizations \cite{kuo2023understanding, krafft2021action, saxena2020conducting}. Other approaches involve creating new spaces of interactions via open workshops \cite{encodejustice}, engaging online via thorough documentation of AI systems and their effects \cite{lucy}, or developing frameworks that communities can utilize to regulate technologies \cite{policingprojectnyu}. All these methods offer ways to think about how we can create sites for explaining with predictive tools that not just welcome but prioritize the voices of those most harmed by predictive tools. 

\section{Designing explaining media} 

\textit{What forms can explaining take?} When designing systems for explaining, XAI researchers must consider the media through which explaining and knowing can happen. Effective explaining media can support publics in accessing and understanding AI systems in relation to their lives and contexts. 

Most public-facing explanations of AI systems exist as white papers documenting their workings, websites documenting the products, media articles describing the products, their use, and their effects, or in forms of proprietary information in institutions that may (or may not) be accessible via (Freedom of Information Act) FOIA requests. Most of these forms are difficult for publics to access and include languages that require people be skilled in understanding these systems with respect to their goals \cite{robertson2021modeling}. Some forms that may be easy to access may be too simplistic and may rarely, if ever, provide useful explanations. Much information about predictive systems may be undocumented and unpublished, and may not even exist in forms that can be retrieved by the public.

\subsection{Our approach, learning, and challenges}

In the beginning of this research, I, as a member of the public, attempted to engage with existing modalities that offer AI explanations including locating relevant papers and reports, filing FOIA requests, researching media articles and contacting journalists, and analyzing the works of other scholars and activists who have studied place based predictive policing. I hit several roadblocks. Rarely, if ever, the technology makers documented their processes for public access. My FOIA requests were denied and I had little legal knowledge to challenge authorities. When reviewing public articles, I encountered contrasting information where some information may be dated or incorrect. For example, while one article reported the suspension of Predpol in Atlanta, another said it was still in use.  A journalist I spoke with as part of the study in \Cref{ch2:gee} empathized and declared that she had to stop studying and writing about these tools due to a lack of access to understandable information. I was unable to access people and institutions who may know more about these products. The few people I did hear back from, including a now-retired police officer in East Atlanta, knew little about the workings or use of these tools. These efforts to access existing media through which explanations of AI systems could be consumed lasted for around 4 months and produced limited results. The little information I gained by reviewing academic \cite{brayne2020predict} and activist work \cite{lucy} were then translated into languages of my designed media. I believed that my design of media would promote access and understanding for diverse publics. This work employed two primary media to situate the explaining and knowing in publics' contexts: mapping and workshops. 

\textbf{Mapping.} I used maps/mapping as tools/techniques to engage with the workings of predictive systems. Maps offered a tool to think about AI systems in grounded ways that were centered in the neighborhoods where participants lived and worked. Participants were familiar with the history and culture of places on the maps in ways that helped them imagine how predictive systems may categorize spaces. Additionally, grounding the discussion in participants’ lives allowed them to identify spatial, systemic, and social factors that will influence the workings of the predictive systems. The discussions were documented on a large shared map using markers.

\textbf{Participatory Workshops.}  Explanations were created and shared through participatory workshops. In my preliminary research involving interviews with communities, participants suggested the use of websites, videos, online lectures and panels, as potential media. These media, people suggested would allow the transfer of information to be easier to distribute, more efficient to consume, and easier to access. However, even though these forms already exist, publics remain unaware of the workings of civic AI systems. These forms are also not interactive. Lastly, they put us, as researchers, in the position of being ‘experts’ and having the ability to explain these systems to communities based on their needs. 

To address these limits, my work employed participatory mapping workshops guided by a loose protocol to co-develop systemic explanations of AI. I designed for shared power in the development of explanations and actively considered everyone in the room an expert who brings in their local knowledges to help us collectively understand the workings and impacts of predictive systems. 

\subsection{Opportunities and challenges }

Using maps as the medium guiding the explanations allowed us to understand “AI in place”. Maps helped spatially organize the thoughts of diverse audiences. They kept the explanations grounded in spaces that participants knew and lived in. As we would expect to see in participatory workshops, participants led the conversation often building on each other's thoughts or challenging them. Participants brought in diverse local knowledges to pluralistically explain AI \cite{klein2024data}. Bringing those knowledges together, on a shared map, resulted in the group being able to compare different neighborhoods in relation to the disparate algorithmic effects they may feel.

Both these media helped develop grounded critical thinking in participants and workshop organizers. However, they had their  limits. Sometimes participants were not strongly connected to their neighborhoods and therefore were not able to draw on their knowledge of the spaces they lived in. They may be unaware or new to a space. Very often people preferred talking about spaces with maps as a reference instead of marking their explanations on the map. Marking on the map seemed time-intensive and would break the flow of conversation. This made it difficult to document their discussions and partial explanations. In the participatory workshops, at times I was placed in an authoritative role where the participants looked at us to educate them. I tried to reset this power imbalance by informing participants of our goal to collectively understand predictive systems. Other times, participants came in with strong opinions and ideas about the role of AI in society. It was not always possible to engage with those ideas productively, especially if it meant challenging them. The explanations were limited to the fixed time, space, and capacity of the workshops \cite{rosner2016out} and the constraints of the maps and its projected data layers. 

\subsection{Other inspirations}

XAI research need not merely focus on providing neat, structured explanations of complex entangled predictive systems \cite{inman2019beautiful}. The form and medium of explaining can reflect and provide opportunities to engage with the complex ways in which predictive tools organize cities \cite{mattern2020city, gupta2022rethinking}. Understanding AI systems may happen through a wide variety of media including the use of simulations, models, or tinkering with a tool (where the tool itself is the medium). Even though these techniques may not always result in accurate explanations of how a given AI system works, they can represent AI systems in relatable ways making them easy to understand \cite{paez2019pragmatic}. Visualization, as well as physicalization of AI, have been proposed as methods that can support critical engagement with AI systems \cite{ghajargar2021explainable}.

More recently, XAI researchers are calling for explanations to move away from being individual-centric to being community-centric. Some researchers have shown how explanations can be `socially constructed' such as through conversations on community forums that support users in collectively explaining how an AI platform works and affects them \cite{kou2020mediating} . Even in the adjacent field of data literacy, researchers have called for, project-based learning and hands- on peer-learning \cite{d2018creative}. An increasing number of researchers are calling for interactive explanations or dialogical explanations \cite{cheng2019explaining, crisan2022interactive}. Such explanations allow users to ask follow-up questions to an automated agent. 

Both techniques of mapping and participatory workshops were motivated by my goals to ground explanations in publics' environment (via mapping) and city's broader environment (via participatory workshops). 

 
\section{Designing explaining interactions }

\textit{What interactions support practices of explaining?} The interaction techniques and protocols that guide the process of explaining and knowing affect the creation of situated and systemic explanations. Interfaces, and the interactions they support for explaining, must consider who gets to explain, the constraints they encounter, and their effects on our collective understandings of AI systems. 

Public facing explanation of civic AI systems tend to appear as a one-way transfer of information through webpages or news articles. The dis-embodied nature of these interactions treats the goals of explanations as accessing information, not knowing or understanding AI systems. It also does not support the broader move towards promoting the public understanding of
science and AI by producing a `literate citizen' \cite{datasocprimer}. These interactions aim to prescribe the feelings of trust or fear instead of promoting independent critical thinking or training people to seek out relevant explanations. 

\subsection{Our approach}

\textbf{Personating.} The workshops invited participants to personate the working of the AI system and mark on a map the spatial predictions they think an AI system may make, i.e. participants were invited to `step into the shoes of place-based predictive policing' and categorize spaces as `high crime' or `low crime' on a map. In the pilot sessions, some participants reported that they felt `uncomfortable' making predictions as they realized the socio-political assumptions they made and the discriminatory stereotypes that drove those assumptions, for example assuming that low wealth neighborhoods may see high crime, thereby labeling poor people `criminals'. This helped them understand the emotional responsibility a decision-maker feels in making these decisions and if and how AI systems skirt this essential responsibility \cite{veliz2021moral}. 

Later, the workshop groups discussed why they categorized places a certain way in the personation exercise. As they engaged in this exercise, they reasoned about how an AI system may work by reflecting on their predictions. For example, when assigned the task of predicting, many participants first asked `what kinds of crime are we talking about', which led to the group discussing how AI system's predictions may differ based on the kinds of crime they focus on. They were asked to imagine how their reasoning for marking places as high crime may be reflected in existing data sets and how these datasets may or may not serve as \textit{true} reflections of ground reality. Such discussions led to people discussing the pros and cons of 911 data, arrest data, crowdsourced data, and socio-economic data. They were asked to then consider how their predictions divide space and the physical boundaries they create. Participants noticed that some of their predictions were at the scale of an intersection, while others were at the scale of neighborhoods. They considered if and how AI tools divide spaces in ways that may cause irreparable damage, not unlike the racial segregation created by red lining maps in in the 1930s \cite{aaronson2021effects}. Participants also considered the time-scale of data that went into their own predictions. They realized that for some areas, their predictions were based on much recent data, while for other areas, which they perceived as `hardly changed', their data was from a decade ago. Sometimes, they realized that their predictions were based on older data that may not be valid because of the growth and development in the neighborhood. Lastly, many participants talked about how these tools affect spaces in real life. They reflected on the effects of labeling spaces as `high crime'  on the economic investment that an area attracts, the people who live there or frequent the area, and the relationships of people with each other. 

\textbf{Protocols}. Even as I attempted to abide by the ethos of participatory research, I realized the need to have prompts that can help guide the conversation. These prompts guided the personation process described above. I noticed in the pilots, that the more components of the AI system I revealed, the better the participants were able to relate it back to their expertise. It supported participants in identifying their expertise domain in relation to the predictive tool. The protocol involved having people map out the places they thought would be marked as high crime by an AI system (as described in the personation section above). At times when the conversation slowed down, I introduced components such as prediction type (What is the tool predicting?), Data Type (What kinds of data is the tool using?), Data Selection (How much data and from what sources is being used?), Data Aggregation (How is the data being aggregated in space?), and Prediction Impacts (How does the prediction impact space). This is detailed in the toolkit shared in the Appendix C. I may not have the accurate explanation of an AI component. For example, I may not know what kinds of data are fed into place-based predictive policing systems or what the source of the data is. Yet, merely introducing this as an aspect that informs AI workings allowed the group to speculate, understand, and explain how these tools may work in their neighborhoods and how they mat affect their communities. 

\subsection{Opportunities and challenges }

Personating AI helped participants understand AI workings by reflecting on the decision-making and reasoning that implicitly happens when they themselves make predictions. Such reflection helped identify questions and concerns AI systems may have to engage with as they make predictions. Personating required workshop coordinators to provide prompts that triggered the reflective process. These prompts consisted of revealing AI components that inform AI workings. These were loosely followed but were essential to guide the participants to think about their prediction in relation to AI.  Personating AI predictions focused on the socio-technical workings of AI—what happens that allows AI to make predictions. We engaged in additional speculative exercises to consider how AI systems impact space—what happens after AI makes predictions.

Our prototcols were flexible and participants at many instances introduced algorithmic components that may be more relevant or concerning to them. The protocol allowed us to introduce explanation points (tiny pieces of information similar to data points) that the participants could then relate to in their local contexts. There were struggles. The protocols developed from one workshop to another as I learned what AI components are most relatable. However, since this work was not done in close collaboration with one social group, I may not have offered appropriate prompts to diverse groups and their local contexts. 

\subsection{Other inspirations}

Some works introduce interactions that follow the lead of Seymour Papert who proposed `learning by doing'  \cite{papert1993children}. Participatory Art has been proposed as a strategy to support public interaction with civic, often `invisible' AI \cite{blair2021participatory}.  Other modes of creative interactions such as through the use of activity boxes \cite{long2023fostering} have shown promise in explaining AI. 

Visual languages including familiar signs, symbols, and icons have been used to make AI legible in accessible and user-friendly manners \cite{lindley2020researching}. Additionally, thought experiments such as forward engineering AI systems, in contrast to reverse engineering, have been proposed to proactively help us understand algorithmic impacts \cite{polack2020beyond}. Critique as a mechanism of `AI transparency'  has helped uncover discriminatory power structures that form the foundation of AI \cite{hollanek2023ai}.  

Other projects such as the Moral Machine by MIT \cite{awad2018moral} provide users with a platform to imagine how AI system could or should work by taking the place of AI and making judgments about the movement of an automated car in `trolley car'-like problems. Personation has been considered an effective method for understanding the workings of other complex technological systems by others. Seymour Papert developed the coding language LOGO to teach students maths and computation by following the perspective and trail of a turtle on a screen. Students were able to draw on their knowledge of their own bodies to imagine how a turtle would move \cite{papert2020mindstorms}. Another example is discussed in an ethnography written by Janet Vertesi who demonstrates how scientists used gestures and the materials around them on earth to make sense of how the Mars Rover would see its environment and operate on Mars. They used this sense to code the rover's movements \cite{vertesi2012seeing}. 

We believe that a thorough consideration of the design dimensions noted above—designing sites, designing media, and designing interactions— can support the creation of systems of explaining that are situated, systemic, continuous and partial. The reader of this dissertation may have noticed that I have not yet discussed the fourth quality of good enough explanations that I conceptualized in \Cref{ch2:gee}, i.e. good enough explanations are actionable. So, how can good enough explanations be placed in sites, shared through  media, and consumed through interactions, that promote public action? This has been a difficult question to answer in the course of this PhD with limited time and resources. Yet, I share some of my reflections below and discuss paths forward. 

\section{Good enough explaining goals and following public actions }

In \Cref{ch2:gee}, I found that good enough explanations are not good enough by themselves. They are good enough in so far as they can support public action. \textit{How then can explaining support public action?}

Through this research, I realized that publics do not always have specific goals or actions that motivate their search for explanations. Participants in the workshops noted in their survey responses their desire to learn more about `AI' or make sense of their concerns as their \textit{goal} (detailed in \cref{ch3:methods}).  Oftentimes, a lack of knowledge and understanding may prevent the conceptualization of effective public actions. Publics may not be aware that a situation necessitates an action if they are not aware that there is, in fact, a `situation' made up of civic AI tools that is affecting them in invisible ways. Introducing partial explanations, however, including simply making publics aware of the presence of place-based predictive tools, initiated publics' desire to act. I observed instances of this in my work. A participant in Workshop 1 who works as part of a police diversion group, stated that now, having learned how predictions work, they want to aim their team's diversion efforts to places predicted as having `high crime' with the forethought that the people there may be most in need of their group's assistance. Another participant in Workshop 3 wanted to share learnings from the workshop in civic meetings to guide civic funding decisions. Some educators in Workshop 5 discussed the need to build critical AI literacy for their students in this age of rapid AI growth. As such, one of the goals of good enough explaining can be to support the formulation of publics' desire to act, transforming passive citizens into active and literate systems required for the functioning of a healthy democracy \cite{ballard2016action}.  

Another observation was the publics' desire to come together in a room to talk about predictive systems, share concerns, and learn. Dazzled by the growth in AI around them, they welcomed sites where they can gather and talk in an organized but low stakes manner. This was felt by the organizers not only during the workshops that happened and the discussions that preceded and followed them, but also through the workshops that did not happen. I was in talks with four other groups in Atlanta who recognized the need to come together to learn about AI and were enthusiastic about participating. One organization proposed long term collaboration to offer such workshops to the community members they work with as part of the `academic' pillar and another offered to collaborate to design panel-like learning experiences for policy-makers. Unfortunately, despite my excitement and desire, I was unable to organize additional workshops due to resource constraints. What I did see however was the formation of publics around these issues. That, I believe, is another communal action that explanation sites can achieve. As Le Dantec explains in his book `Designing Publics' \cite{le2016designing}, publics can take shape over time in the course of collective design practices (in our case collective explanation practices). Even though the timeline of this work was much shorter, I see \textit{designing publics} around responsible use of civic AI systems as an action that good enough explaining can attempt to achieve. 

In the course of the workshops, I also heard participants express their inability to \textit{act}. A participant in Workshop 1 asked if ``all we can do is vent". My position as an individual researcher separate from the civic system (choice of site) gave me limited ability to provide the participants with direct pathways to action. Louder \textit{venting}, however, can be seen as a necessary start. 

Another project that I contributed to during my PhD offers a valuable example of a site designed to promote public action. The project designed a curriculum called `Youth Advocacy for Resilience to Disasters(YARDs)' that used Mapspot \cite{Mapspot} to conduct participatory mapping with young BIPOC students. The week-long program, conducted as part of summer camps, supported middle school students in drawing on spatial data to understand the effects of natural disasters on their communities. Next, it prompted students to propose infrastructural improvement projects to build resilience in face of natural disasters. Lastly, it provided them an opportunity to advocate for changes by presenting their proposed improvements to local experts and public officials. The program deliberately designed a site to promote \textit{action} such as  advocating for infrastructural change. Such interactions with local and city leaders can be built into explaining sites to deliberately promote public action. 

The dissertation work is limited in its ability to promote direct public action towards redesigning, challenging, or discontinuing AI systems. I was also unable to study long term effects of the workshops on public action. The scope of the work was limited to promoting public actions such as reflection, discussion, and debate. In future work, I would like to encourage deliberate formulation of public actions and document ways in which participants can directly act in service of their goals through civic, systemic, or public channels. 

